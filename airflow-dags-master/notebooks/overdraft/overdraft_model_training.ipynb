{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overdraft model training notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing relevant libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../.')\n",
    "\n",
    "import pickle\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from overdraft_prediction.model import ODModel\n",
    "from overdraft_prediction.fine_tuner import FineTuner, ActionType, ThresholdType\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import snowflake.connector as sf\n",
    "import xgboost as xgb\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../../.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting pandas preferred display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#set preferances\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "warnings.filterwarnings(action='once')\n",
    "sns.set(rc={'axes.facecolor': 'white', 'figure.facecolor': 'white'})\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to SnowFlake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "conn = sf.connect(user=os.getenv('SNOWFLAKE_USER'),\n",
    "                  password=os.getenv('SNOWFLAKE_PASSWORD'),\n",
    "                  account=os.getenv('SNOWFLAKE_HOST')\n",
    "                  )\n",
    "\n",
    "\n",
    "def run_query(connection, query):\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(query)\n",
    "    cursor.close()\n",
    "\n",
    "\n",
    "snowflakecursor = conn.cursor()\n",
    "try:\n",
    "    sql = 'alter warehouse {} resume'.format(os.getenv('SNOWFLAKE_WAREHOUSE'))\n",
    "    run_query(conn, sql)\n",
    "except:\n",
    "    pass\n",
    "sql = 'use database {}'.format(os.getenv('SNOWFLAKE_DATABASE'))\n",
    "run_query(conn, sql)\n",
    "sql = 'use database {}'.format(os.getenv('SNOWFLAKE_DATABASE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the functions to create training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_monthly_training_dateset(date_to_run):\n",
    "    fd = date_to_run\n",
    "        \n",
    "    first_table_cols = '''\n",
    "    select bank_account_id, count(distinct cl.id) as login_count\n",
    "    from (select bank_account_id, customer_id from DWH.fact_mysql_customer_monthly group by 1,2) dw\n",
    "    join \"LILI_ANALYTICS_DEV\".\"ODS\".\"MYSQL_CUSTOMER_LOGIN\" cl on cl.customer_id=dw.customer_id\n",
    "    where create_time>=' ''' + fd + ''' ' and create_time<=dateadd(month, 1,' ''' + fd + ''' ')\n",
    "    group by bank_account_id\n",
    "    '''\n",
    "    customer_login_per_date_range = pd.read_sql(first_table_cols, conn)\n",
    "\n",
    "    second_table_cols = '''\n",
    "    select BANK_ACCOUNT_ID, sum(TRANSACTION_AMOUNT) as total_money_in1\n",
    "    from \"LILI_ANALYTICS\".\"DWH\".\"FACT_MYSQL_ACCOUNT_TRANSACTION_ALL\"\n",
    "    where transaction_date>dateadd(month, 1, ' ''' + fd + ''' ') and transaction_date<=dateadd(month, 2,' ''' + fd + ''' ')\n",
    "    and act_type in ('PM','AD') and transaction_amount>0\n",
    "    group by BANK_ACCOUNT_ID\n",
    "    '''\n",
    "    \n",
    "    bank_account_did_transactions = pd.read_sql(second_table_cols, conn)\n",
    "    \n",
    "    mcc_table_cols = '''\n",
    "    select BANK_ACCOUNT_ID, MCC_CODE, sum(TRANSACTION_AMOUNT) as money_amount\n",
    "    from \"LILI_ANALYTICS\".\"DWH\".\"FACT_MYSQL_ACCOUNT_TRANSACTION_ALL\"\n",
    "    where transaction_date>=' ''' + fd + ''' ' and transaction_date<=dateadd(month, 1,' ''' + fd + ''' ')\n",
    "    and TRANSACTION_AMOUNT<0\n",
    "    group by BANK_ACCOUNT_ID, MCC_CODE\n",
    "    '''\n",
    "    mcc_table = pd.read_sql(mcc_table_cols, conn)\n",
    "    mcc_table = pd.crosstab(mcc_table.BANK_ACCOUNT_ID, mcc_table.MCC_CODE, values=mcc_table.MONEY_AMOUNT, aggfunc='sum').fillna(0)\n",
    "\n",
    "    pending_check_cols = '''\n",
    "    SELECT bank_account_id,count(case when status=40 then id else null end ) as pending_check\n",
    "    from  \"LILI_ANALYTICS_DEV\".\"ODS\".\"MYSQL_ACCOUNT_PENDING_CHECK\"\n",
    "    where  BANK_ACCOUNT_ID is not null\n",
    "    and CREATE_TIME>=' ''' + fd + ''' ' and CREATE_TIME<=dateadd(month, 1,' ''' + fd + ''' ')\n",
    "    group by 1\n",
    "    '''\n",
    "    \n",
    "    pending_check = pd.read_sql(pending_check_cols, conn)\n",
    "    \n",
    "    \n",
    "    trans_rej_cols = '''\n",
    "    select bank_account_id\n",
    "    ,sum(case when SUB_TYPE='denied_auth_nsf' then abs(TRANSACTION_AMOUNT) else null end ) denied_auth_nsf  \n",
    "    ,count(case when SUB_TYPE='denied_auth_nsf' then bank_account_id else null end ) denied_auth_nsf_cnt \n",
    "    ,sum(case when SUB_TYPE='denied_auth' then abs(TRANSACTION_AMOUNT) else null end ) denied_auth \n",
    "    ,count(case when SUB_TYPE='denied_auth' then bank_account_id else null end ) denied_auth_cnt \n",
    "    ,sum(case when SUB_TYPE='auth_exp' then abs(TRANSACTION_AMOUNT) else null end ) auth_exp  \n",
    "    ,count(case when SUB_TYPE='auth_exp' then bank_account_id else null end ) auth_exp_cnt \n",
    "    ,sum(case when SUB_TYPE='denied_auth_inactive_card' then abs(TRANSACTION_AMOUNT) else null end ) denied_auth_inactive_card  \n",
    "    ,count(case when SUB_TYPE='denied_auth_inactive_card' then bank_account_id else null end ) denied_auth_inactive_card_cnt \n",
    "    ,sum(case when SUB_TYPE='denied_auth_invalid_pin' then abs(TRANSACTION_AMOUNT) else null end ) denied_auth_invalid_pin \n",
    "    ,count(case when SUB_TYPE='denied_auth_invalid_pin' then bank_account_id else null end ) denied_auth_invalid_pin_cnt \n",
    "    ,sum(case when SUB_TYPE='denied_auth_gas' then abs(TRANSACTION_AMOUNT) else null end ) denied_auth_gas\n",
    "    ,count(case when SUB_TYPE='denied_auth_gas' then bank_account_id else null end ) denied_auth_gas_cnt \n",
    "    ,sum(case when SUB_TYPE='ach_credit_fail' then abs(TRANSACTION_AMOUNT) else null end ) ach_credit_fail\n",
    "    ,count(case when SUB_TYPE='ach_credit_fail' then bank_account_id else null end ) ach_credit_fail_cnt\n",
    "    ,sum(case when SUB_TYPE='ach_credit_return' then abs(TRANSACTION_AMOUNT) else null end ) ach_credit_return\n",
    "    ,count(case when SUB_TYPE='ach_credit_return' then bank_account_id else null end ) ach_credit_return_cnt\n",
    "    ,sum(case when SUB_TYPE='create_hold' then abs(TRANSACTION_AMOUNT) else null end ) create_hold\n",
    "    ,count(case when SUB_TYPE='create_hold' then bank_account_id else null end ) create_hold_cnt\n",
    "    ,sum(case when SUB_TYPE='expire_hold' then abs(TRANSACTION_AMOUNT) else null end ) expire_hold\n",
    "    ,count(case when SUB_TYPE='expire_hold' then bank_account_id else null end ) expire_hold_cnt\n",
    "    from \"LILI_ANALYTICS\".\"ODS\".\"MYSQL_ACCOUNT_TRANSACTION_REJECT\"\n",
    "    WHERE TRANSACTION_DATE>=' ''' + fd + ''' ' and TRANSACTION_DATE<=dateadd(month, 1,' ''' + fd + ''' ')\n",
    "    group by 1\n",
    "    '''\n",
    "    trans_rej = pd.read_sql(trans_rej_cols, conn)\n",
    "    \n",
    "    \n",
    "    third_table_cols = '''\n",
    "    select\n",
    "    dw.customer_id,\n",
    "    dw.bank_account_id,\n",
    "    min(signup_date) as signup_date,\n",
    "    dateadd(month, 1,' ''' + fd + ''' ') as period_end,\n",
    "    max(transaction_date) as last_transaction_in_time,\n",
    "    count(case when ata.rolling_balance < 0 then 1 else null end) as had_negative_balance,\n",
    "    max(case when act_type='PM' then transaction_date else null end) as last_transaction_money_in_in_time,\n",
    "    abs(sum(case when act_type in ('ST','VS','IS','VI', 'DB', 'SD', 'MP') and ata.type = 'W' then ata.transaction_amount else 0 end)) as ATM_sum,\n",
    "    abs(sum(case when act_type in ('ST','VS','IS','VI', 'DB', 'SD', 'MP') and ata.type <> 'W' then ata.transaction_amount else 0 end)) as Swipe_sum,\n",
    "    abs(sum(case when act_type in ('ST','VS','IS','VI', 'DB', 'SD', 'MP') then ata.transaction_amount else 0 end)) as Spend_sum,\n",
    "    sum(case when act_type='PM' and ata.type='FM' then abs(ata.transaction_amount) else 0 end) - sum(case when (ata.type='FM' and act_type='AD' and details='Direct Deposit Return') then abs(ata.transaction_amount) else 0 end) as direct_deposit_sum,\n",
    "    sum(case when act_type='PM' and (ata.type in ('VT','VA','VH','MX')) then abs(ata.transaction_amount) else 0 end) as direct_pay_sum,\n",
    "    abs(sum(case when (act_type='PM' and ata.type='AC') and ata.transaction_amount > 0 then ata.transaction_amount else 0 end)) as ACH_sum,\n",
    "    sum(case when act_type='PM' and (ata.type='OR') then abs(ata.transaction_amount) else 0 end) as Check_sum,\n",
    "    sum(case when act_type='PM' and (ata.type in ('GT', 'GO', 'CE')) then abs(ata.transaction_amount) else 0 end) as Greendot_sum,\n",
    "    sum(case when (act_type='AD' and details='Debit Card transfer') then abs(ata.transaction_amount) else 0 end) as Card_Deposit_sum,\n",
    "    sum(case when (act_type='PM' and ata.type<>'C2') or (act_type='AD' and (details='Debit Card transfer' or ata.type='FM')) and transaction_amount > 1 then transaction_amount else null end) - sum(case when (ata.type='FM' and act_type='AD' and details='Direct Deposit Return') then abs(ata.transaction_amount) else 0 end) as Total_money_in,\n",
    "    sum(case when (act_type='PM' and ata.type<>'C2') or (act_type='AD' and (details='Debit Card transfer' or ata.type='FM')) and transaction_amount > 1 and transaction_date >= dateadd(day, 14, ' ''' + fd + ''' ') then transaction_amount else null end) - sum(case when (ata.type='FM' and act_type='AD' and details='Direct Deposit Return') then abs(ata.transaction_amount) else 0 end) as Total_money_in2,\n",
    "    count(case when (act_type='PM' and ata.type<>'C2') or (act_type='AD' and (details='Debit Card transfer' or ata.type='FM')) and transaction_amount > 1 then transaction_amount  else null end) - count(case when (ata.type='FM' and act_type='AD' and details='Direct Deposit Return') then abs(ata.transaction_amount) else null end) as Total_money_in_Count,\n",
    "    avg(ata.rolling_balance) as average_balance,\n",
    "    count(distinct case when dds.type='PAYROLL' then 1 else null end) as did_payroll,\n",
    "    count(distinct case when dds.type='Marketplace' then 1 else null end) as did_marketplace,\n",
    "    count(distinct case when dds.type='FINANCIAL INSTITUTION' then 1 else null end) as did_financial_institution,\n",
    "    count(distinct case when dds.type='Unemployment' then 1 else null end) as did_unemployment,\n",
    "    count(distinct case when dds.type='Tax Refund' then 1 else null end) as did_tax_refund\n",
    "    from (select customer_id, bank_account_id, signup_date, account_active from DWH.fact_mysql_customer_monthly group by 1,2,3,4) dw\n",
    "    join DWH.fact_mysql_account_transaction_all ata on dw.bank_account_id=ata.bank_account_id and ata.transaction_date > ' ''' + fd + ''' ' and ata.transaction_date <= dateadd(month, 1, ' ''' + fd + ''' ') \n",
    "    left join ODS.mysql_direct_deposit_sources dds on dds.merchant=ata.details\n",
    "    where account_active=1\n",
    "    group by 1, 2\n",
    "    having Total_money_in>500\n",
    "    order by 1'''\n",
    "\n",
    "    money_in_week1_cols = '''\n",
    "    select\n",
    "    bank_account_id,\n",
    "    sum(case when (act_type='PM' and type<>'C2') or (act_type='AD' and (details='Debit Card transfer' or type='FM')) and transaction_amount > 1 then transaction_amount else null end) - sum(case when (type='FM' and act_type='AD' and details='Direct Deposit Return') then abs(transaction_amount) else 0 end) as Total_money_in_week1,\n",
    "    count(case when (act_type='PM' and type<>'C2') or (act_type='AD' and (details='Debit Card transfer' or type='FM')) and transaction_amount > 1 then transaction_amount  else null end) - count(case when (type='FM' and act_type='AD' and details='Direct Deposit Return') then abs(transaction_amount) else null end) as Total_money_in_Count_week1\n",
    "    from DWH.fact_mysql_account_transaction_all\n",
    "    where transaction_date >= ' ''' + fd + ''' ' and transaction_date <= dateadd(day, -21, dateadd(month, 1, ' ''' + fd + ''' '))\n",
    "    group by 1\n",
    "    order by 1\n",
    "    '''\n",
    "    \n",
    "    money_in_week1 = pd.read_sql(money_in_week1_cols, conn)    \n",
    "    \n",
    "    \n",
    "    money_in_week2_cols = '''\n",
    "    select\n",
    "    bank_account_id,\n",
    "    sum(case when (act_type='PM' and type<>'C2') or (act_type='AD' and (details='Debit Card transfer' or type='FM')) and transaction_amount > 1 then transaction_amount else null end) - sum(case when (type='FM' and act_type='AD' and details='Direct Deposit Return') then abs(transaction_amount) else 0 end) as Total_money_in_week2,\n",
    "    count(case when (act_type='PM' and type<>'C2') or (act_type='AD' and (details='Debit Card transfer' or type='FM')) and transaction_amount > 1 then transaction_amount  else null end) - count(case when (type='FM' and act_type='AD' and details='Direct Deposit Return') then abs(transaction_amount) else null end) as Total_money_in_Count_week2\n",
    "    from DWH.fact_mysql_account_transaction_all\n",
    "    where transaction_date > dateadd(day, -21, dateadd(month, 1, ' ''' + fd + ''' ')) and transaction_date <= dateadd(day, -14, dateadd(month, 1, ' ''' + fd + ''' '))\n",
    "    group by 1\n",
    "    order by 1\n",
    "    '''\n",
    "    \n",
    "    money_in_week2 = pd.read_sql(money_in_week2_cols, conn)\n",
    "    \n",
    "    money_in_week3_cols = '''\n",
    "    select\n",
    "    bank_account_id,\n",
    "    sum(case when (act_type='PM' and type<>'C2') or (act_type='AD' and (details='Debit Card transfer' or type='FM')) and transaction_amount > 1 then transaction_amount else null end) - sum(case when (type='FM' and act_type='AD' and details='Direct Deposit Return') then abs(transaction_amount) else 0 end) as Total_money_in_week3,\n",
    "    count(case when (act_type='PM' and type<>'C2') or (act_type='AD' and (details='Debit Card transfer' or type='FM')) and transaction_amount > 1 then transaction_amount  else null end) - count(case when (type='FM' and act_type='AD' and details='Direct Deposit Return') then abs(transaction_amount) else null end) as Total_money_in_Count_week3\n",
    "    from DWH.fact_mysql_account_transaction_all\n",
    "    where transaction_date > dateadd(day, -14, dateadd(month, 1, ' ''' + fd + ''' ')) and transaction_date <= dateadd(day, -7, dateadd(month, 1, ' ''' + fd + ''' '))\n",
    "    group by 1\n",
    "    order by 1\n",
    "    '''\n",
    "    \n",
    "    money_in_week3 = pd.read_sql(money_in_week3_cols, conn)\n",
    "    \n",
    "    money_in_week4_cols = '''\n",
    "    select\n",
    "    bank_account_id,\n",
    "    sum(case when (act_type='PM' and type<>'C2') or (act_type='AD' and (details='Debit Card transfer' or type='FM')) and transaction_amount > 1 then transaction_amount else null end) - sum(case when (type='FM' and act_type='AD' and details='Direct Deposit Return') then abs(transaction_amount) else 0 end) as Total_money_in_week4,\n",
    "    count(case when (act_type='PM' and type<>'C2') or (act_type='AD' and (details='Debit Card transfer' or type='FM')) and transaction_amount > 1 then transaction_amount  else null end) - count(case when (type='FM' and act_type='AD' and details='Direct Deposit Return') then abs(transaction_amount) else null end) as Total_money_in_Count_week4\n",
    "    from DWH.fact_mysql_account_transaction_all\n",
    "    where transaction_date > dateadd(day, -7, dateadd(month, 1, ' ''' + fd + ''' ')) and transaction_date <= dateadd(month, 1, ' ''' + fd + ''' ') \n",
    "    group by 1\n",
    "    order by 1\n",
    "    ''' \n",
    "    \n",
    "    money_in_week4 = pd.read_sql(money_in_week4_cols, conn)\n",
    "    \n",
    "    third_table = pd.read_sql(third_table_cols, conn)\n",
    "    first_join = third_table.merge(customer_login_per_date_range, on=['BANK_ACCOUNT_ID'], how='left')\n",
    "    second_join = first_join.merge(pending_check, on=['BANK_ACCOUNT_ID'], how='left')\n",
    "    third_join = second_join.merge(trans_rej, on=['BANK_ACCOUNT_ID'], how='left')\n",
    "    bank_account_did_transactions = bank_account_did_transactions.merge(mcc_table, on=['BANK_ACCOUNT_ID'], how='left')\n",
    "    trans_table = third_join.merge(bank_account_did_transactions, on=['BANK_ACCOUNT_ID'], how='left')\n",
    "    table_with_money_week1 = trans_table.merge(money_in_week1, on=['BANK_ACCOUNT_ID'], how='left')\n",
    "    table_with_money_week2 = table_with_money_week1.merge(money_in_week2, on=['BANK_ACCOUNT_ID'], how='left')\n",
    "    table_with_money_week3 = table_with_money_week2.merge(money_in_week3, on=['BANK_ACCOUNT_ID'], how='left')\n",
    "    table_all = table_with_money_week3.merge(money_in_week4, on=['BANK_ACCOUNT_ID'], how='left')\n",
    "    table_all.rename(columns={'TOTAL_MONEY_IN1': 'LABEL'}, inplace=True)\n",
    "    table_all['LABEL'] = table_all['LABEL'].fillna(0)\n",
    "    table_all['LOGIN_COUNT'] = table_all['LOGIN_COUNT'].fillna(0)\n",
    "    table_all['TOTAL_MONEY_IN'] = table_all['TOTAL_MONEY_IN'].fillna(0)\n",
    "    return table_all\n",
    "\n",
    "\n",
    "\n",
    "def create_complete_training_set(start_date, end_date):\n",
    "    date_dt_from = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    date_dt_to = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    future_date = date_dt_from\n",
    "    table_all = []\n",
    "    i = 0\n",
    "    while future_date <= date_dt_to:\n",
    "        print(f\"Currently creating dataset of date: {str(future_date)}\")\n",
    "        current_date = str(future_date)\n",
    "        current_date = current_date[0:-9]\n",
    "        table_all.append(create_monthly_training_dateset(current_date))\n",
    "        \n",
    "        future_date = future_date + relativedelta(months=1)\n",
    "        i = i + 1\n",
    "    \n",
    "    #regrouping all tables together and postprocessing\n",
    "    sum = 0\n",
    "    temp = table_all[0]\n",
    "    for j in range(0, i):\n",
    "        sum = sum + table_all[j].shape[0]\n",
    "        if j >= 1:\n",
    "            temp = pd.concat([temp, table_all[j]], ignore_index=True)\n",
    "    print(f\"total table columns: {str(sum)}\")\n",
    "\n",
    "\n",
    "\n",
    "    table_all = temp\n",
    "    table_all['DID_NONE'] = (1 - table_all[\n",
    "        ['DID_PAYROLL', 'DID_MARKETPLACE', 'DID_FINANCIAL_INSTITUTION', 'DID_UNEMPLOYMENT', 'DID_TAX_REFUND']].max(\n",
    "        axis=1))\n",
    "    table_all2 = table_all\n",
    "\n",
    "    table_all['TIME_FROM_LAST_TRANSACTION'] = \\\n",
    "    (pd.to_datetime(table_all['PERIOD_END'], format=\"%Y%m%\") - table_all['LAST_TRANSACTION_IN_TIME']) \\\n",
    "        .astype('timedelta64[D]') \n",
    "    table_all['TIME_FROM_LAST_MONEY_IN'] = \\\n",
    "    (pd.to_datetime(table_all['PERIOD_END'], format=\"%Y%m%\") - table_all['LAST_TRANSACTION_MONEY_IN_IN_TIME']) \\\n",
    "        .astype('timedelta64[D]')\n",
    "    table_all = table_all.drop(['SIGNUP_DATE', 'CUSTOMER_ID', 'BANK_ACCOUNT_ID', 'PERIOD_END', 'LAST_TRANSACTION_IN_TIME', \\\n",
    "                            'LAST_TRANSACTION_MONEY_IN_IN_TIME'], axis=1)\n",
    "    table_all['AVG_MONEY_IN'] = np.where(table_all['TOTAL_MONEY_IN'] == 0.00, 0.00, \n",
    "                                     table_all['TOTAL_MONEY_IN'] / table_all['TOTAL_MONEY_IN_COUNT'])\n",
    "\n",
    "    table_all.fillna(0)\n",
    "    table_all= table_all.replace(np.nan,0)\n",
    "    return table_all\n",
    "\n",
    "\n",
    "def reorder_and_finalize_dataset(dataset):\n",
    "    #make dataset columns in specific order and add mcc codes including missing ones\n",
    "    #from MCC_LIST_FOR_OVERDRAFT table\n",
    "    \n",
    "    cols = list(table_all.columns)\n",
    "    cols.sort()\n",
    "\n",
    "    mcc_list_cols = '''\n",
    "    select * from  \"LILI_ANALYTICS_DEV\".\"ODS\".\"MCC_LIST_FOR_OVERDRAFT\" order by MCC_CODE'''\n",
    "    mcc_list = pd.read_sql(mcc_list_cols, conn)\n",
    "    list1 = ['TOTAL_MONEY_IN', 'TOTAL_MONEY_IN_COUNT', 'TOTAL_MONEY_IN_WEEK1',\\\n",
    "    'TOTAL_MONEY_IN_COUNT_WEEK1', 'TOTAL_MONEY_IN_WEEK2',\\\n",
    "    'TOTAL_MONEY_IN_COUNT_WEEK2', 'TOTAL_MONEY_IN_WEEK3', \\\n",
    "    'TOTAL_MONEY_IN_COUNT_WEEK3', 'TOTAL_MONEY_IN_WEEK4',\\\n",
    "    'TOTAL_MONEY_IN_COUNT_WEEK4', 'HAD_NEGATIVE_BALANCE', 'ATM_SUM', 'SWIPE_SUM',\\\n",
    "    'SPEND_SUM', 'DIRECT_DEPOSIT_SUM', 'DIRECT_PAY_SUM', 'ACH_SUM', 'CHECK_SUM', 'GREENDOT_SUM',\\\n",
    "    'CARD_DEPOSIT_SUM', 'AVERAGE_BALANCE', 'DID_PAYROLL', 'DID_MARKETPLACE', 'DID_FINANCIAL_INSTITUTION',\\\n",
    "    'DID_UNEMPLOYMENT', 'DID_TAX_REFUND', 'DID_NONE', 'LOGIN_COUNT', 'PENDING_CHECK', 'DENIED_AUTH_NSF',\\\n",
    "    'DENIED_AUTH_NSF_CNT', 'DENIED_AUTH', 'DENIED_AUTH_CNT', 'AUTH_EXP', 'AUTH_EXP_CNT', \\\n",
    "    'DENIED_AUTH_INACTIVE_CARD', 'DENIED_AUTH_INACTIVE_CARD_CNT', 'DENIED_AUTH_INVALID_PIN',\\\n",
    "    'DENIED_AUTH_INVALID_PIN_CNT', 'DENIED_AUTH_GAS', 'DENIED_AUTH_GAS_CNT', 'ACH_CREDIT_FAIL',\\\n",
    "    'ACH_CREDIT_FAIL_CNT', 'ACH_CREDIT_RETURN', 'ACH_CREDIT_RETURN_CNT', 'CREATE_HOLD', 'CREATE_HOLD_CNT',\\\n",
    "    'EXPIRE_HOLD', 'EXPIRE_HOLD_CNT', 'TIME_FROM_LAST_TRANSACTION', 'TIME_FROM_LAST_MONEY_IN', 'AVG_MONEY_IN',\\\n",
    "    'LABEL']\n",
    "\n",
    "    num_all = -len(list1)-1\n",
    "    current_mccs = cols[:num_all]\n",
    "\n",
    "    missing_mcc = list(set(mcc_list['MCC_CODE']) - set(current_mccs))\n",
    "\n",
    "    for elt in missing_mcc:\n",
    "        table_all[elt] = 0\n",
    "    list2 = list(mcc_list['MCC_CODE'].iloc[0:])\n",
    "    listush = list1 + list2\n",
    "    table_ready = table_all[listush]\n",
    "    return table_ready\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_from = '2020-04-01'\n",
    "date_to = '2021-07-01' #taking 3 at most months back from current month\n",
    "table_all = create_complete_training_set(date_from, date_to)\n",
    "table_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some additional necessary functions for later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def money_loss(labels, predictions, balance_up_amounts):\n",
    "    money = 0\n",
    "    for i in range(0, len(labels)):\n",
    "        if predictions[i]==4 and labels[i]==0:\n",
    "            money = money + (balance_up_amounts[4] - balance_up_amounts[0])\n",
    "        if predictions[i]==4 and labels[i]==1:\n",
    "            money = money + (balance_up_amounts[4] - balance_up_amounts[1])\n",
    "        if predictions[i]==4 and labels[i]==2:\n",
    "            money = money + (balance_up_amounts[4] - balance_up_amounts[2])\n",
    "        if predictions[i]==4 and labels[i]==3:\n",
    "            money = money + (balance_up_amounts[4] - balance_up_amounts[3])\n",
    "        if predictions[i]==3 and labels[i]==0:\n",
    "            money = money + (balance_up_amounts[3] - balance_up_amounts[0])\n",
    "        if predictions[i]==3 and labels[i]==1:\n",
    "            money = money + (balance_up_amounts[3] - balance_up_amounts[1])\n",
    "        if predictions[i]==3 and labels[i]==2:\n",
    "            money = money + (balance_up_amounts[3] - balance_up_amounts[2])\n",
    "        if predictions[i]==2 and labels[i]==0:\n",
    "            money = money + (balance_up_amounts[2] - balance_up_amounts[0])\n",
    "        if predictions[i]==2 and labels[i]==1:\n",
    "            money = money + (balance_up_amounts[2] - balance_up_amounts[1])\n",
    "        if predictions[i]==1 and labels[i]==0:\n",
    "            money = money + (balance_up_amounts[1] - balance_up_amounts[0])\n",
    "    return money\n",
    "\n",
    "def adjust_output_to_labels(label_feature, overdraft_percent_dict):\n",
    "    #currently the specs are as follows 20 - 50%, 40 - 30%, 60 - 15%, 100 - 5%\n",
    "    ranges = []\n",
    "    percents = []\n",
    "    od_values = list(overdraft2percent.keys())\n",
    "    for elt in od_values:\n",
    "        percents.append(overdraft2percent[elt])\n",
    "    #add the first element from the keys so being under this threshold will be considered 0 overdraft\n",
    "    ranges.append(od_values[0])\n",
    "    specs = label_feature.describe(np.linspace(0, 1, 101))\n",
    "    for i in range(0,len(specs)):\n",
    "        for j in range(1,len(percents)):\n",
    "            if specs.keys()[i][0:-1]==str(np.sum(percents[0:j])):\n",
    "                ranges.append(int(specs.values[i]))\n",
    "    return ranges\n",
    "\n",
    "def convert_label_to_numerical_ranges(dataset, label_ranges):\n",
    "    labeler = dataset['LABEL']\n",
    "    for i in range(0, len(labeler)):\n",
    "        \n",
    "        if labeler[i] < label_ranges[0]:\n",
    "            labeler[i]=0\n",
    "            continue\n",
    "        for j in range(1, len(label_ranges)):\n",
    "            if labeler[i] >= label_ranges[j-1] and labeler[i]<label_ranges[j]:\n",
    "                labeler[i]=j\n",
    "                continue\n",
    "        if labeler[i]>=label_ranges[-1]:\n",
    "            labeler[i]=len(label_ranges)\n",
    "    return labeler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalizing dataset and defining label ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary between overdraft and precentage from users money-in for prediction\n",
    "overdraft2percent = {20: 50, 40: 30, 60: 15, 100: 5}\n",
    "\n",
    "table_general = reorder_and_finalize_dataset(table_all)\n",
    "table_general\n",
    "\n",
    "label_outputs = adjust_output_to_labels(table_general['LABEL'], overdraft2percent)\n",
    "print(f\"ranges for balance-up values in USD are: {label_outputs}\")\n",
    "labelers = convert_label_to_numerical_ranges(table_general, label_outputs)\n",
    "table_general['LABEL'] = labelers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check for ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumlate = []\n",
    "for i in range(0, len(overdraft2percent)+1):\n",
    "    cumlate.append(table_general[table_general['LABEL'] == i].shape[0])\n",
    "    print(f\"For label {i} we have {cumlate[-1]} date points\")\n",
    "    \n",
    "print(f\"Total labeled ranges are {np.sum(cumlate)}\") \n",
    "\n",
    "if np.sum(cumlate)!=table_general.shape[0]:\n",
    "    print(\"Error! Something is wrong. Total number of date point does not equal to labeled ranges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features for training with train-test division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Let's modelize!\n",
    "\n",
    "#First step - make seperate labels from features and convert to numpy arrays\n",
    "\n",
    "# Labels are the values we want to predict\n",
    "labels = np.array(table_general['LABEL'])\n",
    "# Remove the labels from the features\n",
    "# axis 1 refers to the columns\n",
    "features = table_general.drop('LABEL', axis=1)\n",
    "# Saving feature names for later use\n",
    "feature_list = list(features.columns)\n",
    "# Convert to numpy array\n",
    "features = np.array(features)\n",
    "features.shape\n",
    "# Split to training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model using XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Third step - train model\n",
    "\n",
    "model_overdraft = xgb.XGBClassifier(use_label_encoder=False, max_depth=3, n_estimators=250, learning_rate=0.3)\n",
    "model_overdraft.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check model accuracy and relative accuracy (equal or lower prediction) and present the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In particular, the relative accuracy is in the lower triagular values of the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions = model_overdraft.predict(test_features)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(predictions, test_labels))\n",
    "\n",
    "comparush = (predictions <= test_labels)\n",
    "unique, counts = np.unique(comparush, return_counts=True)\n",
    "print(\"The predictions portions that were equal or less: \" + str(counts[1] / (counts[0] + counts[1])))\n",
    "#the current algorithm gives 85.5% of clients overdraft less or equal to what they should\n",
    "disp = plot_confusion_matrix(model_overdraft, test_features, test_labels, display_labels=['0', '20', '40', '60', '100'],\n",
    "                             cmap=plt.cm.Blues, values_format='.1f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Present feature importance for this model training (from highest to lowest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(model_overdraft.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key=lambda x: x[1], reverse=True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];\n",
    "\n",
    "# Set the style\n",
    "plt.style.use('fivethirtyeight')\n",
    "# list of x locations for plotting\n",
    "x_values = list(range(len(importances)))\n",
    "# Make a bar chart\n",
    "plt.bar(x_values, importances, orientation='vertical')\n",
    "# Tick labels for x axis\n",
    "plt.xticks(x_values, feature_list, rotation='vertical')\n",
    "# Axis labels and title\n",
    "plt.ylabel('Importance');\n",
    "plt.xlabel('Variable');\n",
    "plt.title('Variable Importances');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding shap explainer object for model reason (not obligatory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model_overdraft, train_features, feature_perturbation=\"interventional\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding finetune object for model reason (not obligatory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alittle bit more information regarding finetune. This object helps the model be a little bit more careful on some situations and alittle bit more hesitant on others. The way the finetune model works is, it takes the prediction probabilities and sets thresholds on these probabilities, such that when a prediction probability threshold is set, the output will change accordingly. For additional information, check the test_fine_tuner.py file, there are good examples there. To find good thresholds, it can be a good practice to run the money_loss function defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_fine_tuner1 = FineTuner(0, 0.3, ThresholdType.MORE_THAN, ActionType.NEXT_BEST)\n",
    "example_fine_tuner2 = FineTuner(1, 0.55, ThresholdType.LESS_THAN, ActionType.ABSOLUTE, 3)\n",
    "\n",
    "current_overdraft_limits = [0, 20, 40, 60, 100]\n",
    "loss_usd = money_loss(test_labels, predictions, current_overdraft_limits)\n",
    "print(f\"money currently lost is {loss_usd}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE!!! Model rejection, although exists as object is currently not used. I believe that in the future it will be used, this is why in the ODModel object it can be used in input. For reference and some code reusability, I kept the old model reject code down below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model to pkl file. ODModel inputs are: model_money_in, model_reject (Not obligatory), finetuner (Not obligatory), explainer  (Not obligatory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "#creating and saving the ODmodel\n",
    "#currently we just put balance-up model and explainer object as inputs\n",
    "odmodel = ODModel(model_money_in=model_overdraft, explainer=explainer)\n",
    "filename = f'../../model_dumps/ODmodel_{datetime.datetime.utcnow().isoformat()}.pkl'\n",
    "pickle.dump(odmodel, open(filename, 'wb'))\n",
    "print(f\"model is saved in {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if 'explainerc' in vars() or 'explainerc' in globals():\n",
    "    print(\"exists\")\n",
    "else:\n",
    "    print(\"does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odmodel.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Load a model if necessary\n",
    "modeler = pickle.load(open('../../model_dumps/ODmodel_2021-08-17T15:05:04.740356.pkl', 'rb'))\n",
    "explainush = modeler.explainer\n",
    "print(dir(modeler))\n",
    "modeler.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "print(metrics.classification_report(test_labels, predictions))\n",
    "disp = plot_confusion_matrix(model_overdraft, test_features, test_labels, display_labels=['0', '20', '40', '60', '100'],\n",
    "                             cmap=plt.cm.Blues, values_format='.1f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(0, len(predictions)):\n",
    "    if test_labels[i]==2 and predictions[i]==3:\n",
    "        count = count + 1\n",
    "print(count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#predictions2.shape\n",
    "preds_proba = model_overdraft.predict_proba(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions[preds_proba[:, 0] > 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(labels)\n",
    "sns.heatmap(confusion_matrix(test_labels, predictions), cmap=\"Blues\", annot=True, fmt=\".2f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "preds_proba[preds_proba[:, 0] > 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(test_labels, predictions, target_names=['200', '100', '50', '20', 'NO']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(test_labels, predictions, target_names=['200', '100', '50', '20', 'NO']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Let's take the five highest and try to run the algorithm again\n",
    "table2 = table_ready[['LABEL', 'TIME_FROM_LAST_TRANSACTION',\n",
    "                      'TOTAL_MONEY_IN',\n",
    "                      'TOTAL_MONEY_IN_COUNT',\n",
    "                      'DID_UNEMPLOYMENT',\n",
    "                      'DIRECT_DEPOSIT_SUM',\n",
    "                      'DID_TAX_REFUND', 'DID_PAYROLL', 'DID_NONE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Let's modelize!\n",
    "\n",
    "#First step - make seperate labels from features and convert to numpy arrays\n",
    "\n",
    "# Labels are the values we want to predict\n",
    "labels2 = np.array(table2['LABEL'])\n",
    "# Remove the labels from the features\n",
    "# axis 1 refers to the columns\n",
    "features2 = table2.drop('LABEL', axis=1)\n",
    "# Saving feature names for later use\n",
    "feature_list2 = list(features2.columns)\n",
    "# Convert to numpy array\n",
    "features2 = np.array(features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Second step - split the data into training and testing sets\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features2, labels2, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Third step - train model\n",
    "\n",
    "\n",
    "model = xgb.XGBClassifier(max_depth=3, n_estimators=250, learning_rate=0.24)\n",
    "model.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_features)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(predictions, test_labels))\n",
    "comparush = (predictions >= test_labels)\n",
    "unique, counts = np.unique(comparush, return_counts=True)\n",
    "print(\"the predictions that were at least equal or worse are \" + str(counts[1] / (counts[0] + counts[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "disp = plot_confusion_matrix(model_overdraft, test_features, test_labels, display_labels=['NO', '20', '40', '60', '100'],\n",
    "                             cmap=plt.cm.Blues, values_format='.1f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "\n",
    "for i in range(0,len(test_features)):\n",
    "    if predictions[i]==1 and test_labels[i]==2:\n",
    "        count=count+1\n",
    "        \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(test_labels, predictions, target_names=['NO', '20', '40', '60', '100']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "table_ready['LABEL'].hist(bins=50)\n",
    "\n",
    "table_ready['LABEL'].describe(np.linspace(0, 1, 51))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "preds_proba = model_overdraft.predict_proba(test_features)\n",
    "ranger = np.arange(start=0.20, stop=0.55, step=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lst = []\n",
    "for i in range(0, len(test_features)):\n",
    "    chosen_cat = np.argmax(preds_proba[i])\n",
    "    if chosen_cat==3:\n",
    "        lst.append(max(preds_proba[i]))\n",
    "\n",
    "lst = np.array(lst)\n",
    "print(np.min(lst))\n",
    "print(np.median(lst))\n",
    "print(np.mean(lst))\n",
    "print(np.max(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = model_overdraft.predict(test_features)\n",
    "for i in range(0, len(test_features)):\n",
    "    chosen_cat = np.argmax(preds_proba[i])\n",
    "    if chosen_cat==4 and max(preds_proba[i])>0.525:\n",
    "        predictions2[i] = np.argmax([preds_proba[i][0],preds_proba[i][1],preds_proba[i][2],preds_proba[i][3]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(ranger, full[0])\n",
    "plt.xlabel(\"Prediction Probability Threshold\")\n",
    "plt.ylabel(\"Probability for true prediction above threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(ranger, full[1])\n",
    "plt.xlabel(\"Prediction Probability Threshold\")\n",
    "plt.ylabel(\"Probability for true prediction above threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(ranger, full[2])\n",
    "plt.xlabel(\"Prediction Probability Threshold\")\n",
    "plt.ylabel(\"Probability for true prediction above threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(ranger, full[3])\n",
    "plt.xlabel(\"Prediction Probability Threshold\")\n",
    "plt.ylabel(\"Probability for true prediction above threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(ranger, full[4])\n",
    "plt.xlabel(\"Prediction Probability Threshold\")\n",
    "plt.ylabel(\"Probability for true prediction above threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#fine-tuning for the model\n",
    "predictions2 = model.predict(test_features)\n",
    "comparush = (predictions >= test_labels)\n",
    "unique, counts = np.unique(comparush, return_counts=True)\n",
    "orig_eob = counts[1] / (counts[0] + counts[1])\n",
    "orig_acc = metrics.accuracy_score(predictions, test_labels)\n",
    "print(orig_acc)\n",
    "print(orig_eob)\n",
    "rng = np.arange(start=0.3, stop=0.65, step=0.02)\n",
    "twks = []\n",
    "for j in range(0, len(rng)):\n",
    "    for k in range(0, len(rng)):\n",
    "        for l in range(0, len(rng)):\n",
    "            for i in range(0, len(preds_proba)):\n",
    "                chosen_cat = np.argmax(preds_proba[i]) - 5\n",
    "                prob = max(preds_proba[i])\n",
    "                if prob <= rng[j] and chosen_cat == -5:\n",
    "                    predictions2[i] = np.argmax(\n",
    "                        [preds_proba[i][1], preds_proba[i][2], preds_proba[i][3], preds_proba[i][4]]) - 4\n",
    "                if prob <= rng[k] and chosen_cat == -4:\n",
    "                    predictions2[i] = np.argmax([preds_proba[i][2], preds_proba[i][3], preds_proba[i][4]]) - 3\n",
    "                if prob <= rng[l] and chosen_cat == -3:\n",
    "                    predictions2[i] = np.argmax([preds_proba[i][3], preds_proba[i][4]]) - 2\n",
    "            new_acc = metrics.accuracy_score(predictions2, test_labels)\n",
    "            comparush2 = (predictions2 >= test_labels)\n",
    "            unique, counts = np.unique(comparush2, return_counts=True)\n",
    "            new_eob = counts[1] / (counts[0] + counts[1])\n",
    "            #print(\"for threshold of [{:01f},{:01f},{:01f}] we get acc diff of {:02f} and eob diff of {:02f}.\".format(rng[j],rng[k],rng[l], (new_acc-orig_acc)*100,\\\n",
    "            #                                (new_eob-orig_eob)*100))\n",
    "            #print(\"ratio is {:02f}\".format((new_eob-orig_eob)/(new_acc-orig_acc)))\n",
    "            #print(new_acc)\n",
    "            #print(new_eob)\n",
    "            twks.append([rng[j], rng[k], rng[l], new_acc, new_eob])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(preds_proba)):\n",
    "    chosen_cat = np.argmax(preds_proba[i]) - 5\n",
    "    prob = max(preds_proba[i])\n",
    "    if prob <= 0.432 and chosen_cat == -5:\n",
    "        predictions2[i] = np.argmax([preds_proba[i][1], preds_proba[i][2], preds_proba[i][3], preds_proba[i][4]]) - 4\n",
    "    if prob <= 0.49 and chosen_cat == -4:\n",
    "        predictions2[i] = np.argmax([preds_proba[i][2], preds_proba[i][3], preds_proba[i][4]]) - 3\n",
    "    if prob <= 0.36 and chosen_cat == -3:\n",
    "        predictions2[i] = np.argmax([preds_proba[i][3], preds_proba[i][4]]) - 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_features)\n",
    "print(\"Original Accuracy:\", metrics.accuracy_score(predictions, test_labels))\n",
    "comparush = (predictions >= test_labels)\n",
    "unique, counts = np.unique(comparush, return_counts=True)\n",
    "print(\"the predictions that were at least equal or worse are \" + str(counts[1] / (counts[0] + counts[1])))\n",
    "\n",
    "print(\"Altered Accuracy:\", metrics.accuracy_score(predictions2, test_labels))\n",
    "comparush = (predictions2 >= test_labels)\n",
    "unique, counts = np.unique(comparush, return_counts=True)\n",
    "print(\"the predictions that were at least equal or worse are \" + str(counts[1] / (counts[0] + counts[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(labels)\n",
    "sns.heatmap(confusion_matrix(test_labels, predictions), cmap=\"Blues\", annot=True, fmt=\".2f\"\n",
    "           , xticklabels=[0,20,40,60,100], yticklabels= [0,20,40,60,100])\n",
    "\n",
    "np.histogram(preds_proba[:, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(labels)\n",
    "sns.heatmap(confusion_matrix(test_labels, predictions2), cmap=\"Blues\", annot=True, fmt=\".2f\"\n",
    "            , xticklabels=[0,20,40,60,100], yticklabels= [0,20,40,60,100])\n",
    "np.histogram(preds_proba[:, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = np.arange(0.5,1,0.01)\n",
    "vec2 = []\n",
    "for elt in vec:\n",
    "    predictions2 = model_overdraft.predict(test_features)\n",
    "    for i in range(0, len(test_features)):\n",
    "        chosen_cat = np.argmax(preds_proba[i])\n",
    "        if chosen_cat==3 and max(preds_proba[i])<elt:\n",
    "            predictions2[i] = np.argmax([preds_proba[i][0],preds_proba[i][1],preds_proba[i][2]])\n",
    "    vec2.append(count_loss(test_labels, predictions2))\n",
    "#print(count_loss(test_labels, predictions))\n",
    "#print(count_loss(test_labels, predictions2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(vec,vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = model_overdraft.predict(test_features)\n",
    "for i in range(0, len(test_features)):\n",
    "    chosen_cat = np.argmax(preds_proba[i])\n",
    "    if chosen_cat==4 and max(preds_proba[i])<0.68:\n",
    "        predictions2[i] = np.argmax([preds_proba[i][0],preds_proba[i][1],preds_proba[i][2],preds_proba[i][3]])\n",
    "    if chosen_cat==3 and max(preds_proba[i])<0.58:\n",
    "        predictions2[i] = np.argmax([preds_proba[i][0],preds_proba[i][1],preds_proba[i][2]])\n",
    "\n",
    "print(count_loss(test_labels, predictions))\n",
    "print(count_loss(test_labels, predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "table_all2['LABEL'].describe(np.linspace(0, 1, 101))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN THIS PART WE TRAIN THE REJECTION MODEL, WHICH TAKES INPUT AND SAYS IF BEHAVIOR OF CUSTOMER IS STRANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OD_cols = ''' select *  from \"LILI_ANALYTICS_DEV\".\"ODS\".\"OVERDRAFT_LIMITS\" '''\n",
    "\n",
    "OD_table = pd.read_sql(OD_cols, conn)\n",
    "#OD_table[OD_table['OVERDRAFT_LIMIT']=='40.0000']\n",
    "OD_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take all relevant dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_cols = '''\n",
    "select DISTINCT EXPIRATION_DATE from \"LILI_ANALYTICS_DEV\".\"ODS\".\"LAST_7_DAYS_OVERDRAFT_USAGE\" \n",
    "'''\n",
    "orig_dates=[]\n",
    "run_dates=[]\n",
    "dates = pd.read_sql(dates_cols, conn)\n",
    "dates = list(dates['EXPIRATION_DATE'])\n",
    "for i in range(0, len(dates)):\n",
    "    run_dates.append(dates[i] + relativedelta(months=-2))\n",
    "    orig_dates.append(dates[i])\n",
    "\n",
    "    \n",
    "for i in range(0, len(orig_dates)):\n",
    "    new_query_cols = '''\n",
    "    select * from \"LILI_ANALYTICS_DEV\".\"ODS\".\"LAST_7_DAYS_OVERDRAFT_USAGE\"\n",
    "    WHERE EXPIRATION_DATE = ''' + str(\"'\" + str(orig_dates[i]) + \"'\")\n",
    "\n",
    "    new_query = pd.read_sql(new_query_cols, conn)\n",
    "    print(new_query.shape[0])\n",
    "    \n",
    "rel_prns_cols = '''\n",
    "select BANK_ACCOUNT_NUMBER from \"LILI_ANALYTICS_DEV\".\"ODS\".\"LAST_7_DAYS_OVERDRAFT_USAGE\" \n",
    "'''\n",
    "\n",
    "rel_prns = pd.read_sql(rel_prns_cols, conn)\n",
    "rel_prns = list(rel_prns['BANK_ACCOUNT_NUMBER'])\n",
    "rel_prns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_rej_all = []\n",
    "for i in range(0, len(run_dates)+1):\n",
    "    if i<len(run_dates):\n",
    "        fd = str(run_dates[i])\n",
    "    else:\n",
    "        fd = '2021-07-01'\n",
    "    first_table_cols = '''\n",
    "        select bank_account_id, count(distinct cl.id) as login_count, max(pro_customer) as pro_customer\n",
    "        from (select bank_account_id, customer_id, max(pro_customer) as pro_customer from DWH.fact_mysql_customer_monthly group by 1,2) dw\n",
    "        join \"LILI_ANALYTICS_DEV\".\"ODS\".\"MYSQL_CUSTOMER_LOGIN\" cl on cl.customer_id=dw.customer_id\n",
    "        where create_time>=' ''' + fd + ''' ' and create_time<=dateadd(month, 1,' ''' + fd + ''' ')\n",
    "        group by bank_account_id\n",
    "    '''\n",
    "    customer_login_per_date_range = pd.read_sql(first_table_cols, conn)\n",
    "\n",
    "    \n",
    "    mcc_table_cols = '''\n",
    "        select BANK_ACCOUNT_ID, MCC_CODE, sum(TRANSACTION_AMOUNT) as money_amount\n",
    "        from \"LILI_ANALYTICS\".\"DWH\".\"FACT_MYSQL_ACCOUNT_TRANSACTION_ALL\"\n",
    "        where transaction_date>=' ''' + fd + ''' ' and transaction_date<=dateadd(month, 1,' ''' + fd + ''' ')\n",
    "        and TRANSACTION_AMOUNT<0\n",
    "        group by BANK_ACCOUNT_ID, MCC_CODE\n",
    "    '''\n",
    "    mcc_table = pd.read_sql(mcc_table_cols, conn)\n",
    "    mcc_table = pd.crosstab(mcc_table.BANK_ACCOUNT_ID, mcc_table.MCC_CODE, values=mcc_table.MONEY_AMOUNT, aggfunc='sum').fillna(0)\n",
    "\n",
    "    pending_check_cols = '''\n",
    "    SELECT bank_account_id,count(case when status=40 then id else null end ) as pending_check\n",
    "    from  \"LILI_ANALYTICS_DEV\".\"ODS\".\"MYSQL_ACCOUNT_PENDING_CHECK\"\n",
    "    where  BANK_ACCOUNT_ID is not null\n",
    "    and CREATE_TIME>=' ''' + fd + ''' ' and CREATE_TIME<=dateadd(month, 1,' ''' + fd + ''' ')\n",
    "    group by 1\n",
    "    '''\n",
    "    \n",
    "    pending_check = pd.read_sql(pending_check_cols, conn)\n",
    "    \n",
    "    \n",
    "    trans_rej_cols = '''\n",
    "    select bank_account_id\n",
    "    ,sum(case when SUB_TYPE='denied_auth_nsf' then abs(TRANSACTION_AMOUNT) else null end ) denied_auth_nsf  \n",
    "    ,count(case when SUB_TYPE='denied_auth_nsf' then bank_account_id else null end ) denied_auth_nsf_cnt \n",
    "    ,sum(case when SUB_TYPE='denied_auth' then abs(TRANSACTION_AMOUNT) else null end ) denied_auth \n",
    "    ,count(case when SUB_TYPE='denied_auth' then bank_account_id else null end ) denied_auth_cnt \n",
    "    ,sum(case when SUB_TYPE='auth_exp' then abs(TRANSACTION_AMOUNT) else null end ) auth_exp  \n",
    "    ,count(case when SUB_TYPE='auth_exp' then bank_account_id else null end ) auth_exp_cnt \n",
    "    ,sum(case when SUB_TYPE='denied_auth_inactive_card' then abs(TRANSACTION_AMOUNT) else null end ) denied_auth_inactive_card  \n",
    "    ,count(case when SUB_TYPE='denied_auth_inactive_card' then bank_account_id else null end ) denied_auth_inactive_card_cnt \n",
    "    ,sum(case when SUB_TYPE='denied_auth_invalid_pin' then abs(TRANSACTION_AMOUNT) else null end ) denied_auth_invalid_pin \n",
    "    ,count(case when SUB_TYPE='denied_auth_invalid_pin' then bank_account_id else null end ) denied_auth_invalid_pin_cnt \n",
    "    ,sum(case when SUB_TYPE='denied_auth_gas' then abs(TRANSACTION_AMOUNT) else null end ) denied_auth_gas\n",
    "    ,count(case when SUB_TYPE='denied_auth_gas' then bank_account_id else null end ) denied_auth_gas_cnt \n",
    "    ,sum(case when SUB_TYPE='ach_credit_fail' then abs(TRANSACTION_AMOUNT) else null end ) ach_credit_fail\n",
    "    ,count(case when SUB_TYPE='ach_credit_fail' then bank_account_id else null end ) ach_credit_fail_cnt\n",
    "    ,sum(case when SUB_TYPE='ach_credit_return' then abs(TRANSACTION_AMOUNT) else null end ) ach_credit_return\n",
    "    ,count(case when SUB_TYPE='ach_credit_return' then bank_account_id else null end ) ach_credit_return_cnt\n",
    "    ,sum(case when SUB_TYPE='create_hold' then abs(TRANSACTION_AMOUNT) else null end ) create_hold\n",
    "    ,count(case when SUB_TYPE='create_hold' then bank_account_id else null end ) create_hold_cnt\n",
    "    ,sum(case when SUB_TYPE='expire_hold' then abs(TRANSACTION_AMOUNT) else null end ) expire_hold\n",
    "    ,count(case when SUB_TYPE='expire_hold' then bank_account_id else null end ) expire_hold_cnt\n",
    "    from \"LILI_ANALYTICS\".\"ODS\".\"MYSQL_ACCOUNT_TRANSACTION_REJECT\"\n",
    "    WHERE TRANSACTION_DATE>=' ''' + fd + ''' ' and TRANSACTION_DATE<=dateadd(month, 1,' ''' + fd + ''' ')\n",
    "    group by 1\n",
    "    '''\n",
    "    trans_rej = pd.read_sql(trans_rej_cols, conn)\n",
    "    \n",
    "    \n",
    "    third_table_cols = '''\n",
    "    select\n",
    "    dw.customer_id,\n",
    "    dw.bank_account_id,\n",
    "    min(signup_date) as signup_date,\n",
    "    dateadd(month, 1,' ''' + fd + ''' ') as period_end,\n",
    "    max(transaction_date) as last_transaction_in_time,\n",
    "    count(case when ata.rolling_balance < 0 then 1 else null end) as had_negative_balance,\n",
    "    max(case when act_type='PM' then transaction_date else null end) as last_transaction_money_in_in_time,\n",
    "    abs(sum(case when act_type in ('ST','VS','IS','VI', 'DB', 'SD', 'MP') and ata.type = 'W' then ata.transaction_amount else 0 end)) as ATM_sum,\n",
    "    abs(sum(case when act_type in ('ST','VS','IS','VI', 'DB', 'SD', 'MP') and ata.type <> 'W' then ata.transaction_amount else 0 end)) as Swipe_sum,\n",
    "    abs(sum(case when act_type in ('ST','VS','IS','VI', 'DB', 'SD', 'MP') then ata.transaction_amount else 0 end)) as Spend_sum,\n",
    "    sum(case when act_type='PM' and ata.type='FM' then abs(ata.transaction_amount) else 0 end) - sum(case when (ata.type='FM' and act_type='AD' and details='Direct Deposit Return') then abs(ata.transaction_amount) else 0 end) as direct_deposit_sum,\n",
    "    sum(case when act_type='PM' and (ata.type in ('VT','VA','VH','MX')) then abs(ata.transaction_amount) else 0 end) as direct_pay_sum,\n",
    "    abs(sum(case when (act_type='PM' and ata.type='AC') and ata.transaction_amount > 0 then ata.transaction_amount else 0 end)) as ACH_sum,\n",
    "    sum(case when act_type='PM' and (ata.type='OR') then abs(ata.transaction_amount) else 0 end) as Check_sum,\n",
    "    sum(case when act_type='PM' and (ata.type in ('GT', 'GO', 'CE')) then abs(ata.transaction_amount) else 0 end) as Greendot_sum,\n",
    "    sum(case when (act_type='AD' and details='Debit Card transfer') then abs(ata.transaction_amount) else 0 end) as Card_Deposit_sum,\n",
    "    sum(case when (act_type='PM' and ata.type<>'C2') or (act_type='AD' and (details='Debit Card transfer' or ata.type='FM')) and transaction_amount > 1 then transaction_amount else null end) - sum(case when (ata.type='FM' and act_type='AD' and details='Direct Deposit Return') then abs(ata.transaction_amount) else 0 end) as Total_money_in,\n",
    "    count(case when (act_type='PM' and ata.type<>'C2') or (act_type='AD' and (details='Debit Card transfer' or ata.type='FM')) and transaction_amount > 1 then transaction_amount  else null end) - count(case when (ata.type='FM' and act_type='AD' and details='Direct Deposit Return') then abs(ata.transaction_amount) else null end) as Total_money_in_Count,\n",
    "    avg(ata.rolling_balance) as average_balance,\n",
    "    count(distinct case when dds.type='PAYROLL' then 1 else null end) as did_payroll,\n",
    "    count(distinct case when dds.type='Marketplace' then 1 else null end) as did_marketplace,\n",
    "    count(distinct case when dds.type='FINANCIAL INSTITUTION' then 1 else null end) as did_financial_institution,\n",
    "    count(distinct case when dds.type='Unemployment' then 1 else null end) as did_unemployment,\n",
    "    count(distinct case when dds.type='Tax Refund' then 1 else null end) as did_tax_refund\n",
    "    from (select customer_id, bank_account_id, signup_date, account_active from DWH.fact_mysql_customer_monthly group by 1,2,3,4) dw\n",
    "    join DWH.fact_mysql_account_transaction_all ata on dw.bank_account_id=ata.bank_account_id and ata.transaction_date > ' ''' + fd + ''' ' and ata.transaction_date <= dateadd(month, 1, ' ''' + fd + ''' ') \n",
    "    left join ODS.mysql_direct_deposit_sources dds on dds.merchant=ata.details\n",
    "    where account_active=1\n",
    "    group by 1, 2\n",
    "    order by 1'''\n",
    "\n",
    "\n",
    "    added_bank_account_num_cols = '''\n",
    "    SELECT BANK_ACCOUNT_ID, \n",
    "            MAX(BANK_ACCOUNT_NUMBER) AS BANK_ACCOUNT_NUMBER, \n",
    "            MAX((CASE WHEN PRODUCT_ID = 20643 THEN 1 ELSE 0 END)) AS IS_PRO_SINCE_SIGNUP \n",
    "    FROM \"LILI_ANALYTICS\".\"ODS\".\"MYSQL_DW_CUSTOMER_MONTHLY_NEW\"\n",
    "    GROUP BY BANK_ACCOUNT_ID\n",
    "    '''\n",
    "    prn_table = pd.read_sql(added_bank_account_num_cols, conn)\n",
    "\n",
    "\n",
    "    money_in_week1_cols = '''\n",
    "    select\n",
    "    bank_account_id,\n",
    "    sum(case when (act_type='PM' and type<>'C2') or (act_type='AD' and (details='Debit Card transfer' or type='FM')) and transaction_amount > 1 then transaction_amount else null end) - sum(case when (type='FM' and act_type='AD' and details='Direct Deposit Return') then abs(transaction_amount) else 0 end) as Total_money_in_week1,\n",
    "    count(case when (act_type='PM' and type<>'C2') or (act_type='AD' and (details='Debit Card transfer' or type='FM')) and transaction_amount > 1 then transaction_amount  else null end) - count(case when (type='FM' and act_type='AD' and details='Direct Deposit Return') then abs(transaction_amount) else null end) as Total_money_in_Count_week1\n",
    "    from DWH.fact_mysql_account_transaction_all\n",
    "    where transaction_date >= ' ''' + fd + ''' ' and transaction_date <= dateadd(day, -21, dateadd(month, 1, ' ''' + fd + ''' '))\n",
    "    group by 1\n",
    "    order by 1\n",
    "    '''\n",
    "    \n",
    "    money_in_week1 = pd.read_sql(money_in_week1_cols, conn)    \n",
    "    \n",
    "    \n",
    "    money_in_week2_cols = '''\n",
    "    select\n",
    "    bank_account_id,\n",
    "    sum(case when (act_type='PM' and type<>'C2') or (act_type='AD' and (details='Debit Card transfer' or type='FM')) and transaction_amount > 1 then transaction_amount else null end) - sum(case when (type='FM' and act_type='AD' and details='Direct Deposit Return') then abs(transaction_amount) else 0 end) as Total_money_in_week2,\n",
    "    count(case when (act_type='PM' and type<>'C2') or (act_type='AD' and (details='Debit Card transfer' or type='FM')) and transaction_amount > 1 then transaction_amount  else null end) - count(case when (type='FM' and act_type='AD' and details='Direct Deposit Return') then abs(transaction_amount) else null end) as Total_money_in_Count_week2\n",
    "    from DWH.fact_mysql_account_transaction_all\n",
    "    where transaction_date > dateadd(day, -21, dateadd(month, 1, ' ''' + fd + ''' ')) and transaction_date <= dateadd(day, -14, dateadd(month, 1, ' ''' + fd + ''' '))\n",
    "    group by 1\n",
    "    order by 1\n",
    "    '''\n",
    "    \n",
    "    money_in_week2 = pd.read_sql(money_in_week2_cols, conn)\n",
    "    \n",
    "    money_in_week3_cols = '''\n",
    "    select\n",
    "    bank_account_id,\n",
    "    sum(case when (act_type='PM' and type<>'C2') or (act_type='AD' and (details='Debit Card transfer' or type='FM')) and transaction_amount > 1 then transaction_amount else null end) - sum(case when (type='FM' and act_type='AD' and details='Direct Deposit Return') then abs(transaction_amount) else 0 end) as Total_money_in_week3,\n",
    "    count(case when (act_type='PM' and type<>'C2') or (act_type='AD' and (details='Debit Card transfer' or type='FM')) and transaction_amount > 1 then transaction_amount  else null end) - count(case when (type='FM' and act_type='AD' and details='Direct Deposit Return') then abs(transaction_amount) else null end) as Total_money_in_Count_week3\n",
    "    from DWH.fact_mysql_account_transaction_all\n",
    "    where transaction_date > dateadd(day, -14, dateadd(month, 1, ' ''' + fd + ''' ')) and transaction_date <= dateadd(day, -7, dateadd(month, 1, ' ''' + fd + ''' '))\n",
    "    group by 1\n",
    "    order by 1\n",
    "    '''\n",
    "    \n",
    "    money_in_week3 = pd.read_sql(money_in_week3_cols, conn)\n",
    "    \n",
    "    money_in_week4_cols = '''\n",
    "    select\n",
    "    bank_account_id,\n",
    "    sum(case when (act_type='PM' and type<>'C2') or (act_type='AD' and (details='Debit Card transfer' or type='FM')) and transaction_amount > 1 then transaction_amount else null end) - sum(case when (type='FM' and act_type='AD' and details='Direct Deposit Return') then abs(transaction_amount) else 0 end) as Total_money_in_week4,\n",
    "    count(case when (act_type='PM' and type<>'C2') or (act_type='AD' and (details='Debit Card transfer' or type='FM')) and transaction_amount > 1 then transaction_amount  else null end) - count(case when (type='FM' and act_type='AD' and details='Direct Deposit Return') then abs(transaction_amount) else null end) as Total_money_in_Count_week4\n",
    "    from DWH.fact_mysql_account_transaction_all\n",
    "    where transaction_date > dateadd(day, -7, dateadd(month, 1, ' ''' + fd + ''' ')) and transaction_date <= dateadd(month, 1, ' ''' + fd + ''' ') \n",
    "    group by 1\n",
    "    order by 1\n",
    "    ''' \n",
    "    \n",
    "    money_in_week4 = pd.read_sql(money_in_week4_cols, conn)\n",
    "\n",
    "    last_balance_cols = '''\n",
    "    SELECT t.BANK_ACCOUNT_ID, t.CURRENT_BALANCE\n",
    "        FROM \"LILI_ANALYTICS_DEV\".\"ODS\".\"MYSQL_CUSTOMER_BALANCE_HISTORY\" t\n",
    "        INNER JOIN (\n",
    "        SELECT BANK_ACCOUNT_ID, MAX(VALID_DATE) AS MAXDATE\n",
    "        FROM \"LILI_ANALYTICS_DEV\".\"ODS\".\"MYSQL_CUSTOMER_BALANCE_HISTORY\" tm\n",
    "        GROUP BY BANK_ACCOUNT_ID\n",
    "        ) tm ON t.BANK_ACCOUNT_ID = tm.BANK_ACCOUNT_ID AND t.VALID_DATE = tm.MAXDATE\n",
    "    '''\n",
    "\n",
    "    last_balance = pd.read_sql(last_balance_cols, conn)\n",
    "    if i!=len(orig_dates):\n",
    "        labling_cols = '''\n",
    "        select * from \"LILI_ANALYTICS_DEV\".\"ODS\".\"LAST_7_DAYS_OVERDRAFT_USAGE\"\n",
    "        WHERE EXPIRATION_DATE = ''' + str(\"'\" + str(orig_dates[i]) + \"'\")\n",
    "    \n",
    "        labling = pd.read_sql(labling_cols, conn)\n",
    "        labling['LABEL']=1\n",
    "        labling = labling[['LABEL', 'BANK_ACCOUNT_NUMBER','EXPIRATION_DATE']]\n",
    "    third_table = pd.read_sql(third_table_cols, conn)\n",
    "    print(third_table.shape)\n",
    "    first_join  = third_table.merge(customer_login_per_date_range, on=['BANK_ACCOUNT_ID'], how='left')\n",
    "    print(first_join.shape)\n",
    "    second_join = first_join.merge(pending_check, on=['BANK_ACCOUNT_ID'], how='left')\n",
    "    print(second_join.shape)\n",
    "    third_join  = second_join.merge(prn_table, on=['BANK_ACCOUNT_ID'], how='left')\n",
    "    print(third_join.shape)\n",
    "    fourth_join = third_join.merge(trans_rej, on=['BANK_ACCOUNT_ID'], how='left')\n",
    "    fourth_join.BANK_ACCOUNT_NUMBER = fourth_join.BANK_ACCOUNT_NUMBER.astype(np.int64)\n",
    "    print(fourth_join.shape)\n",
    "    fifth_join  = fourth_join.merge(OD_table[['BANK_ACCOUNT_NUMBER','OVERDRAFT_LIMIT']], on=['BANK_ACCOUNT_NUMBER'], how='left')\n",
    "    print(fifth_join.shape)\n",
    "    sixth_join   = fifth_join.merge(last_balance, on=['BANK_ACCOUNT_ID'], how='left')\n",
    "    print(sixth_join.shape)\n",
    "    seventh_join = sixth_join.merge(money_in_week1, on=['BANK_ACCOUNT_ID'], how='left')\n",
    "    eighth_join = seventh_join.merge(money_in_week2, on=['BANK_ACCOUNT_ID'], how='left')\n",
    "    ninth_join = eighth_join.merge(money_in_week3, on=['BANK_ACCOUNT_ID'], how='left')\n",
    "    tenth_join = ninth_join.merge(money_in_week4, on=['BANK_ACCOUNT_ID'], how='left')\n",
    "    table_rej   = tenth_join.merge(mcc_table, on=['BANK_ACCOUNT_ID'], how='left')\n",
    "    print(table_rej.shape)\n",
    "    if i!=len(orig_dates):\n",
    "        table_rej = table_rej.merge(labling, on=['BANK_ACCOUNT_NUMBER'], how='right')\n",
    "        print(table_rej.shape)\n",
    "    else:\n",
    "        table_rej[~table_rej['BANK_ACCOUNT_NUMBER'].isin(rel_prns)]\n",
    "        print(table_rej.shape)\n",
    "    table_rej['DID_NONE'] = (1 - table_rej[\n",
    "        ['DID_PAYROLL', 'DID_MARKETPLACE', 'DID_FINANCIAL_INSTITUTION', 'DID_UNEMPLOYMENT', 'DID_TAX_REFUND']].max(\n",
    "        axis=1))\n",
    "\n",
    "    table_rej['TIME_FROM_LAST_TRANSACTION'] = \\\n",
    "    (pd.to_datetime(table_rej['PERIOD_END'], format=\"%Y%m%\") - table_rej['LAST_TRANSACTION_IN_TIME']) \\\n",
    "        .astype('timedelta64[D]') \n",
    "    table_rej['TIME_FROM_LAST_MONEY_IN'] = \\\n",
    "    (pd.to_datetime(table_rej['PERIOD_END'], format=\"%Y%m%\") - table_rej['LAST_TRANSACTION_MONEY_IN_IN_TIME']) \\\n",
    "        .astype('timedelta64[D]')\n",
    "    #table_rej = table_rej.drop(['SIGNUP_DATE', 'CUSTOMER_ID', 'BANK_ACCOUNT_ID', 'PERIOD_END', 'LAST_TRANSACTION_IN_TIME', \\\n",
    "    #                            'LAST_TRANSACTION_MONEY_IN_IN_TIME'], axis=1)\n",
    "    table_rej['AVG_MONEY_IN'] = np.where(table_rej['TOTAL_MONEY_IN'] == 0.00, 0.00, \n",
    "                                     table_rej['TOTAL_MONEY_IN'] / table_rej['TOTAL_MONEY_IN_COUNT'])\n",
    "\n",
    "    #table_rej = table_rej.drop(['SIGNUP_DATE', 'CUSTOMER_ID', 'BANK_ACCOUNT_ID', 'PERIOD_END', 'LAST_TRANSACTION_IN_TIME', \\\n",
    "    #                        'LAST_TRANSACTION_MONEY_IN_IN_TIME'], axis=1)\n",
    "    table_rej.fillna(0)\n",
    "    table_rej= table_rej.replace(np.nan,0)\n",
    "    print(table_rej.shape)\n",
    "    table_rej = table_rej[table_rej['PRO_CUSTOMER']==1]\n",
    "    print(table_rej.shape)\n",
    "    table_rej = table_rej.drop(['PRO_CUSTOMER'], axis=1)\n",
    "    table_rej.reset_index(drop = True, inplace = True)\n",
    "    table_rej_all.append(table_rej)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating dataframes together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = table_rej_all[0]\n",
    "\n",
    "for i in range(1, len(table_rej_all)):\n",
    "        print(temp.shape)\n",
    "        temp = pd.concat([temp, table_rej_all[i]], ignore_index=True)\n",
    "print(temp.shape)\n",
    "\n",
    "temp = temp.replace(np.nan,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding the label\n",
    "\n",
    "rejected_users_cols = '''\n",
    "select * from \"LILI_ANALYTICS_DEV\".\"ODS\".\"LAST_7_DAYS_OVERDRAFT_USAGE\"\n",
    "'''\n",
    "rejected_users = pd.read_sql(rejected_users_cols, conn)\n",
    "rejected_users['LABEL']=1\n",
    "rejected_users = rejected_users[['LABEL', 'BANK_ACCOUNT_NUMBER','EXPIRATION_DATE']]\n",
    "total_table = temp.merge(rejected_users, on=['BANK_ACCOUNT_NUMBER'], how='left')\n",
    "total_table= total_table.replace(np.nan,0)\n",
    "total_table = total_table.drop(['BANK_ACCOUNT_NUMBER'], axis=1)\n",
    "total_table[total_table['EXPIRATION_DATE']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = ['IS_PRO_SINCE_SIGNUP', 'CURRENT_BALANCE',\n",
    " 'TOTAL_MONEY_IN', 'TOTAL_MONEY_IN_COUNT', 'TOTAL_MONEY_IN_WEEK1',\n",
    " 'TOTAL_MONEY_IN_COUNT_WEEK1', 'TOTAL_MONEY_IN_WEEK2',\n",
    " 'TOTAL_MONEY_IN_COUNT_WEEK2', 'TOTAL_MONEY_IN_WEEK3', 'TOTAL_MONEY_IN_COUNT_WEEK3', \n",
    " 'TOTAL_MONEY_IN_WEEK4', 'TOTAL_MONEY_IN_COUNT_WEEK4', 'HAD_NEGATIVE_BALANCE',\n",
    " 'ATM_SUM','SWIPE_SUM', 'SPEND_SUM','DIRECT_DEPOSIT_SUM','DIRECT_PAY_SUM','ACH_SUM','CHECK_SUM','GREENDOT_SUM',\n",
    " 'CARD_DEPOSIT_SUM','AVERAGE_BALANCE','DID_PAYROLL','DID_MARKETPLACE','DID_FINANCIAL_INSTITUTION',\n",
    " 'DID_UNEMPLOYMENT','DID_TAX_REFUND','DID_NONE','LOGIN_COUNT','PENDING_CHECK','DENIED_AUTH_NSF',\n",
    " 'DENIED_AUTH_NSF_CNT','DENIED_AUTH','DENIED_AUTH_CNT','AUTH_EXP','AUTH_EXP_CNT',\n",
    " 'DENIED_AUTH_INACTIVE_CARD','DENIED_AUTH_INACTIVE_CARD_CNT','DENIED_AUTH_INVALID_PIN',\n",
    " 'DENIED_AUTH_INVALID_PIN_CNT','DENIED_AUTH_GAS','DENIED_AUTH_GAS_CNT','ACH_CREDIT_FAIL',\n",
    " 'ACH_CREDIT_FAIL_CNT','ACH_CREDIT_RETURN','ACH_CREDIT_RETURN_CNT','CREATE_HOLD','CREATE_HOLD_CNT',\n",
    " 'EXPIRE_HOLD','EXPIRE_HOLD_CNT','TIME_FROM_LAST_TRANSACTION','TIME_FROM_LAST_MONEY_IN','AVG_MONEY_IN',\n",
    " 'LABEL']\n",
    "\n",
    "cols = list(temp.columns)\n",
    "cols.sort()\n",
    "#print(cols)\n",
    "\n",
    "\n",
    "mcc_list_cols = '''\n",
    "select * from  \"LILI_ANALYTICS_DEV\".\"ODS\".\"MCC_LIST_FOR_OVERDRAFT\" order by MCC_CODE\n",
    "'''\n",
    "mcc_list = pd.read_sql(mcc_list_cols, conn)\n",
    "#list1 = ['TOTAL_MONEY_IN', 'TOTAL_MONEY_IN_COUNT', 'HAD_NEGATIVE_BALANCE', 'ATM_SUM', 'SWIPE_SUM',\\\n",
    "#    'SPEND_SUM', 'DIRECT_DEPOSIT_SUM', 'DIRECT_PAY_SUM', 'ACH_SUM', 'CHECK_SUM', 'GREENDOT_SUM',\\\n",
    "#    'CARD_DEPOSIT_SUM', 'AVERAGE_BALANCE', 'DID_PAYROLL', 'DID_MARKETPLACE', 'DID_FINANCIAL_INSTITUTION',\\\n",
    "#    'DID_UNEMPLOYMENT', 'DID_TAX_REFUND', 'DID_NONE', 'LOGIN_COUNT', 'PENDING_CHECK', 'DENIED_AUTH_NSF',\\\n",
    "#    'DENIED_AUTH_NSF_CNT', 'DENIED_AUTH', 'DENIED_AUTH_CNT', 'AUTH_EXP', 'AUTH_EXP_CNT', \\\n",
    "#    'DENIED_AUTH_INACTIVE_CARD', 'DENIED_AUTH_INACTIVE_CARD_CNT', 'DENIED_AUTH_INVALID_PIN',\\\n",
    "#    'DENIED_AUTH_INVALID_PIN_CNT', 'DENIED_AUTH_GAS', 'DENIED_AUTH_GAS_CNT', 'ACH_CREDIT_FAIL',\\\n",
    "#    'ACH_CREDIT_FAIL_CNT', 'ACH_CREDIT_RETURN', 'ACH_CREDIT_RETURN_CNT', 'CREATE_HOLD', 'CREATE_HOLD_CNT',\\\n",
    "#    'EXPIRE_HOLD', 'EXPIRE_HOLD_CNT', 'TIME_FROM_LAST_TRANSACTION', 'TIME_FROM_LAST_MONEY_IN', 'AVG_MONEY_IN',\\\n",
    "#    'LABEL']\n",
    "\n",
    "#table_ready = table_all.iloc[:, list]\n",
    "\n",
    "num_all = -len(list1)-1\n",
    "current_mccs = cols[:num_all]\n",
    "\n",
    "missing_mcc = list(set(mcc_list['MCC_CODE']) - set(current_mccs))\n",
    "\n",
    "for elt in missing_mcc:\n",
    "    temp[elt] = 0\n",
    "list2 = list(mcc_list['MCC_CODE'].iloc[0:])\n",
    "listush = list1 + list2\n",
    "final_table = temp[listush]\n",
    "final_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to eliminate a few crucial columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's modelize!\n",
    "\n",
    "#First step - make seperate labels from features and convert to numpy arrays\n",
    "\n",
    "# Labels are the values we want to predict\n",
    "labels = np.array(final_table['LABEL'])\n",
    "# Remove the labels from the features\n",
    "# axis 1 refers to the columns\n",
    "features = final_table.drop('LABEL', axis=1)\n",
    "# Saving feature names for later use\n",
    "feature_list = list(features.columns)\n",
    "# Convert to numpy array\n",
    "features = np.array(features)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second step - split the data into training and testing sets\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, stratify = labels, test_size=0.25)\n",
    "features_res, labels_res = sm.fit_resample(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Third step - train model\n",
    "\n",
    "\n",
    "model_rej = xgb.XGBClassifier(max_depth=3, n_estimators=250, learning_rate=0.24, use_label_encoder=False)\n",
    "model_rej.fit(features_res, labels_res)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_rej.predict(test_features)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(predictions, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count1 = 0\n",
    "count2 = 0\n",
    "for i in range(0,len(labels_res)):\n",
    "    if labels_res[i]==0:\n",
    "        count1=count1+1\n",
    "    else:\n",
    "        count2=count2+1\n",
    "        \n",
    "print(count1)\n",
    "print(count2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(model_rej.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key=lambda x: x[1], reverse=True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];\n",
    "\n",
    "# Set the style\n",
    "plt.style.use('fivethirtyeight')\n",
    "# list of x locations for plotting\n",
    "x_values = list(range(len(importances)))\n",
    "# Make a bar chart\n",
    "plt.bar(x_values, importances, orientation='vertical')\n",
    "# Tick labels for x axis\n",
    "plt.xticks(x_values, feature_list, rotation='vertical')\n",
    "# Axis labels and title\n",
    "plt.ylabel('Importance');\n",
    "plt.xlabel('Variable');\n",
    "plt.title('Variable Importances');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_table['CURRENT_BALANCE'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_table['DAILY_BALANCE'] = total_table['DAILY_BALANCE'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "sns.heatmap(confusion_matrix(test_labels, predictions), cmap=\"Blues\", annot=True, fmt=\".2f\")\n",
    "print(metrics.classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probes = []\n",
    "count = 0\n",
    "ranger = np.arange(0.200,0.501,0.005)\n",
    "predictions2 = model_rej.predict(test_features)\n",
    "preds_proba = model_rej.predict_proba(test_features)\n",
    "confusion_list = []\n",
    "\n",
    "for elt in ranger:\n",
    "    predictions2 = model_rej.predict(test_features)\n",
    "    count_true_positive  = 0\n",
    "    count_true_negative  = 0\n",
    "    count_false_positive = 0\n",
    "    count_false_negative = 0\n",
    "    \n",
    "    for i in range(0, len(predictions)):\n",
    "        if predictions[i]==0 and preds_proba[i][1]>elt:\n",
    "            predictions2[i] = 1\n",
    "            \n",
    "        if predictions2[i]==0 and test_labels[i]==0:\n",
    "            count_true_negative += 1\n",
    "            \n",
    "        if predictions2[i]==0 and test_labels[i]==1:\n",
    "            count_false_negative += 1\n",
    "            \n",
    "        if predictions2[i]==1 and test_labels[i]==1:\n",
    "            count_true_positive += 1\n",
    "            \n",
    "        if predictions2[i]==1 and test_labels[i]==0:\n",
    "            count_false_positive += 1\n",
    "\n",
    "#     print(\"for value \" + str(elt) + \", here are the values:\")\n",
    "# #    print(\"true negative: \" + str(count_true_negative))\n",
    "# #    print(\"true positive: \" + str(count_true_positive))\n",
    "#     print(\"false negative: \" + str(count_false_negative))\n",
    "#     print(\"false negative ratio: \" + str(count_false_negative/(count_false_negative + count_true_positive)))\n",
    "#     print(\"false positive: \" + str(count_false_positive))\n",
    "#     print(\"false positive ratio: \" + str(count_false_positive/(count_false_positive + count_true_negative)))\n",
    "    confusion_list.append([elt, \n",
    "                          count_false_negative/(count_false_negative + count_true_positive),\n",
    "                          count_false_positive/(count_false_positive + count_true_negative)])\n",
    "tf_preditions = pd.DataFrame(confusion_list, columns=['Threshold', 'false_negative_ratio',\n",
    "                                     'false_positive_ratio'])\n",
    "pd.set_option('display.max_rows', None)\n",
    "display(tf_preditions)\n",
    "    #if predictions[i]==0 and preds_proba[i][1]>elt:\n",
    "    #     predictions2[i] = 1\n",
    "        \n",
    "#false positive - I say yes but it is false\n",
    "#false negative - I say no but it is true\n",
    "\n",
    "#count = 0\n",
    "#for i in range(0, len(predictions)):\n",
    "#    if test_labels[i]==1:\n",
    "#        if predictions[i]==0:\n",
    "#            count=count+1\n",
    "#print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_preditions.plot.line(x='Threshold', y='false_negative_ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_preditions.plot.line(x='Threshold', y='false_positive_ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "sns.heatmap(confusion_matrix(test_labels, predictions2), cmap=\"Blues\", annot=True, fmt=\".2f\")\n",
    "print(metrics.classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(model_rej.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key=lambda x: x[1], reverse=True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];\n",
    "\n",
    "# Set the style\n",
    "plt.style.use('fivethirtyeight')\n",
    "# list of x locations for plotting\n",
    "x_values = list(range(len(importances)))\n",
    "# Make a bar chart\n",
    "plt.bar(x_values, importances, orientation='vertical')\n",
    "# Tick labels for x axis\n",
    "plt.xticks(x_values, feature_list, rotation='vertical')\n",
    "# Axis labels and title\n",
    "plt.ylabel('Importance');\n",
    "plt.xlabel('Variable');\n",
    "plt.title('Variable Importances');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list[46]\n",
    "feature_list[2]\n",
    "features_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rej_wrapper = RejectionModel(model_reject = model_rej, threshold = REJECTION_PREDICTION_THRESHOLD)\n",
    "#print(model_rej_wrapper.predict(features_res))\n",
    "rej_explainer = shap.KernelExplainer(model_rej_wrapper.predict, features_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "#saving the ODmodel\n",
    "odmodel = ODModel(model_money_in=model_overdraft, model_reject=model_rej, explainer=explainer, rej_explainer = rej_explainer)\n",
    "filename = f'../../model_dumps/ODmodel_{datetime.datetime.utcnow().isoformat()}.pkl'\n",
    "pickle.dump(odmodel, open(filename, 'wb'))\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeler = pickle.load(open('../../model_dumps/ODmodel_2021-10-12T06:44:43.063140.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeler.rej_explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overdraft_cols = '''\n",
    "select *  from \"LILI_ANALYTICS_DEV\".\"ODS\".\"OVERDRAFT_LIMITS\"\n",
    "'''\n",
    "    \n",
    "overdraft = pd.read_sql(overdraft_cols, conn)\n",
    "\n",
    "\n",
    "overdraft.drop(['BANK_ACCOUNT_ID'], axis=1, inplace=True)\n",
    "\n",
    "bank_account_num_cols = '''\n",
    "select distinct BANK_ACCOUNT_ID, BANK_ACCOUNT_NUMBER FROM \"LILI_ANALYTICS\".\"ODS\".\"MYSQL_DW_CUSTOMER_MONTHLY_NEW\"\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "bank_account_num = pd.read_sql(bank_account_num_cols, conn)\n",
    "bank_account_num.BANK_ACCOUNT_NUMBER = bank_account_num.BANK_ACCOUNT_NUMBER.astype(np.int64)\n",
    "\n",
    "#print(bank_account_num.dtypes)\n",
    "#print(overdraft.dtypes)\n",
    "\n",
    "\n",
    "transaction_cols = '''\n",
    "select BANK_ACCOUNT_ID, max(TRANSACTION_DATE) as max_date from \"LILI_ANALYTICS\".\"ODS\".\"MYSQL_ACCOUNT_TRANSACTION_ALL\"\n",
    "group by BANK_ACCOUNT_ID\n",
    "\n",
    "'''\n",
    "\n",
    "transaction = pd.read_sql(transaction_cols, conn)\n",
    "\n",
    "transaction['TIME_FROM_LAST_TRANSACTION'] = \\\n",
    "    (pd.to_datetime('2021-08-30', format=\"%Y%m%\") - transaction['MAX_DATE']) \\\n",
    "        .astype('timedelta64[D]') \n",
    "\n",
    "\n",
    "balance_cols ='''\n",
    "select t.BANK_ACCOUNT_ID, t.CURRENT_BALANCE\n",
    "        FROM \"LILI_ANALYTICS_DEV\".\"ODS\".\"MYSQL_CUSTOMER_BALANCE_HISTORY\" t\n",
    "        INNER JOIN (\n",
    "        SELECT BANK_ACCOUNT_ID, MAX(VALID_DATE) AS MAXDATE\n",
    "        FROM \"LILI_ANALYTICS_DEV\".\"ODS\".\"MYSQL_CUSTOMER_BALANCE_HISTORY\" tm\n",
    "        GROUP BY BANK_ACCOUNT_ID\n",
    "        ) tm ON t.BANK_ACCOUNT_ID = tm.BANK_ACCOUNT_ID AND t.VALID_DATE = tm.MAXDATE\n",
    "'''\n",
    "\n",
    "balance = pd.read_sql(balance_cols, conn)\n",
    "\n",
    "od_all = overdraft.merge(bank_account_num, on=['BANK_ACCOUNT_NUMBER'], how='left')\n",
    "od_all = od_all[od_all.BANK_ACCOUNT_ID.notnull()].reset_index(drop=True)\n",
    "od_all = od_all.merge(balance, on=['BANK_ACCOUNT_ID'], how='left')\n",
    "od_all = od_all.merge(transaction, on=['BANK_ACCOUNT_ID'], how='left')\n",
    "od_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_all[od_all['OVERDRAFT_LIMIT']!='0.0000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasonizer = od_all[(((od_all['CURRENT_BALANCE'] < -42.5) & (od_all['OVERDRAFT_LIMIT'] == '50.0000'))^\n",
    "       ((od_all['CURRENT_BALANCE'] < -34) & (od_all['OVERDRAFT_LIMIT'] == '40.0000'))^\n",
    "       ((od_all['CURRENT_BALANCE'] < -17) & (od_all['OVERDRAFT_LIMIT'] == '20.0000'))) &\n",
    "      (od_all['TIME_FROM_LAST_TRANSACTION'] >= 7.000) & (od_all['MODEL_REASON'] == 'model rejection')]\n",
    "reasonizer['MODEL_REASON'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_all[od_all['MODEL_REASON']==\"model rejection\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_all[(od_all['MODEL_REASON'] == 'model rejection') & (od_all['OVERDRAFT_LIMIT'] == '50.0000')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MISC model check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = pd.read_csv('../../model_dumps/041021.csv', header=None)\n",
    "today = today.rename(columns={0: \"BANK_ACCOUNT_NUMBER\", 1: \"raw_overdraft\", 2: \"raw_expiration\", 3: \"raw_reason\"})\n",
    "today\n",
    "\n",
    "prev = pd.read_csv('../../model_dumps/prev.csv')\n",
    "\n",
    "\n",
    "today_prod_cols = '''\n",
    "select * from \"LILI_ANALYTICS\".\"ODS\".\"OVERDRAFT_LIMITS\"\n",
    "'''\n",
    "\n",
    "today_prod = pd.read_sql(today_prod_cols, conn)\n",
    "today_merged = today.merge(today_prod[['BANK_ACCOUNT_NUMBER', 'OVERDRAFT_LIMIT', 'EXPIRATION_DATE']], on=['BANK_ACCOUNT_NUMBER'], how='left')\n",
    "today_merged[(today_merged['EXPIRATION_DATE'] == '2021-11-03') & (today_merged['OVERDRAFT_LIMIT'] != '0.0000')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev = prev.rename(columns={\"bank_account_number\": \"BANK_ACCOUNT_NUMBER\", \"expiration_date\": \"prev_expiration\"})\n",
    "prev = prev[['BANK_ACCOUNT_NUMBER', 'prev_expiration']]\n",
    "total_merged = today_merged.merge(prev, on=['BANK_ACCOUNT_NUMBER'], how='left')\n",
    "#total_merged[((total_merged['raw_overdraft']==50) & (total_merged['prev_expiration']=='nan'))]\n",
    "#total_merged[((pd.isna(total_merged['prev_expiration'])) & (total_merged['OVERDRAFT_LIMIT']!='0.0000'))]\n",
    "#total_merged[((pd.isna(total_merged['prev_expiration'])) & (total_merged['raw_overdraft']!=0))]\n",
    "total_merged[((pd.isna(total_merged['prev_expiration']) & (total_merged['raw_overdraft']!=0)))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quer_cols = '''\n",
    "select * from \"LILI_ANALYTICS\".\"DWH\".\"ML_OVERDRAFT_POST_PREDICTION\"\n",
    "where PREDICTION > 0\n",
    "'''\n",
    "\n",
    "quer = pd.read_sql(quer_cols, conn)\n",
    "quer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quer['REASON'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quer2_cols = '''\n",
    "select bank_account_id,\n",
    "(CASE WHEN count(bank_account_id)>0 THEN 1 END) as IS_RECURRENT_DD\n",
    "from LILI_ANALYTICS.ods.MYSQL_DW_CUSTOMER_MONTHLY_NEW dcmn\n",
    "where bank_account_id in (\n",
    "select bank_account_id from (\n",
    "select c.bank_account_number,ata2.bank_account_id,max(total_direct_deposit) as total_direct_deposit,\n",
    "SUM(CASE WHEN (ata2.act_type='PM' AND ata2.type<>'C2') OR (ata2.act_type='AD' AND (ata2.details='Debit Card transfer' OR ata2.type='FM'))\n",
    "THEN ata2.transaction_amount ELSE NULL END) Total_money_in\n",
    "from LILI_ANALYTICS.ods.MYSQL_ACCOUNT_TRANSACTION_ALL as ata2\n",
    "inner join (\n",
    "select bank_account_number,b.bank_account_id, sum(Total_Amount) as total_direct_deposit from\n",
    "(select bank_account_id,bank_account_number\n",
    "from LILI_ANALYTICS.ods.MYSQL_DW_CUSTOMER_MONTHLY_NEW where account_legit =1 and pro_customer=1 group by 1,2) as a\n",
    "inner join (select bank_account_id,\n",
    "case when dss.type is null then 'N/A' else upper(dss.type) end AS Category,\n",
    "sum(transaction_amount) AS Total_Amount\n",
    "from  LILI_ANALYTICS.ods.MYSQL_ACCOUNT_TRANSACTION_ALL AS ata\n",
    "left join LILI_ANALYTICS.ods.MYSQL_DIRECT_DEPOSIT_SOURCES as dss on dss.merchant=ata.details\n",
    "where ata.act_type='PM' and ata.type='FM'\n",
    "And transaction_date >= '2021-08-29' AND transaction_date <= '2021-09-29'\n",
    "group by bank_account_id, dss.type\n",
    "having Category in ('PAYROLL', 'PAYMENTS', 'UNEMPLOYMENT', 'MARKETPLACE','SOCIAL SECURITY ADMINISTRATION','FEDERAL PAYROLL')\n",
    ") as b\n",
    "on a.bank_account_id=b.bank_account_id\n",
    "group by 1,2\n",
    "order by 3 desc) as c\n",
    "on ata2.bank_account_id = c.bank_account_id\n",
    "where transaction_date >='2021-08-29' AND transaction_date <= '2021-09-29'\n",
    "group by 1,2) as d\n",
    "where total_direct_deposit>=500 OR (total_money_in>=200 and total_direct_deposit>=300))\n",
    "group by 1\n",
    "'''\n",
    "\n",
    "\n",
    "quer2 = pd.read_sql(quer2_cols, conn)\n",
    "quer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "df = pd.read_csv('../../model_dumps/checkme.csv', header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../model_dumps/prediction0919.csv', header=None)\n",
    "df = df.rename(columns={0: 'PRN', 1: 'PREDICTION', 2: 'EXPIRATION', 3: 'REASON'})\n",
    "odlim_cols = '''\n",
    "select BANK_ACCOUNT_NUMBER as PRN, OVERDRAFT_LIMIT as OLD_LIMIT, MODEL_REASON as OLD_REASON\n",
    "from \"LILI_ANALYTICS_DEV\".\"ODS\".\"OVERDRAFT_LIMITS\" order by EXPIRATION_DATE desc\n",
    "'''\n",
    "df2 = pd.read_sql(odlim_cols, conn)\n",
    "tbl_all = df.merge(df2, on=['PRN'], how='left')\n",
    "tbl_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_all[(tbl_all['PREDICTION']==50) & (tbl_all['OLD_LIMIT']!='50.0000')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_all[(tbl_all['REASON']=='model rejection')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_all[(tbl_all['REASON']=='model rejection') & (tbl_all['OLD_LIMIT']=='50.0000')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listush = [260101016723,260101019164,260101030880,260101045532,260101052470,260101055739,260101082782,260101098911,260101116044,260101127231,260101142669,260101148039,260101156545,260101170314,260101191567,260101196178,260101210136,260101228880,260101244853,260101262202,260101280071,260101298487,260101322642,260101327161,260101347722,260101349256,260101363448,260101366615,260101387025,260101393627,260101398311,260101399889,260101399897,260101404192,260101412161,260101416808,260101419216,260101424349,260101426229,260101432623,260101433381,260101437796,260101451961,260101456705,260101488179,260101488641,260101489722,260101490878,260101493781,260101497097,260101497808,260101510568,260101510998,260101511251,260101529857,260101533560,260101537611,260101542561,260101548592,260101549657,260101554061,260101557825,260101560316,260101562502,260101569010,260101573053,260101574648,260101575298,260101576700,260101584837,260101585396,260101589687,260101596054,260101602977,260101611663,260101614154,260101614709,260101617843,260101631638,260101635621,260101640894,260101642379,260101646545,260101646875,260101647162,260101655330,260101656155,260101664852,260101670370,260101671162,260101703916,260101709079,260101721199,260101745313,260101745479,260101754638,260101757540,260101761260,260101763050,260101769636,260101770303,260101771111,260101772192,260101780278,260101780302,260101781862,260101785152,260101787513,260101796217,260101803104,260101808335,260101815355,260101827087,260101828580,260101835924,260101842383,260101844504,260101856722,260101860039,260101872620,260101875557,260101890085,260101891893,260101898716,260101906352,260101915528,260101923704,260101933125,260101936946,260101938553,260101955532,260101967818,260101969400,260101975282,260101989937,260101999282,260102006186,260102008497,260102008927,260102011335,260102018413,260102018975,260102024148,260102042272,260102044294,260102047263,260102053949,260102056074,260102056280,260102058724,260102059367,260102065596,260102065778,260102076270,260102077898,260102083870,260102085677,260102086907,260102094117,260102100542,260102109303,260102123718,260102134079,260102134483,260102144623,260102146586,260102147196,260102151040,260102152949,260102157724,260102159225,260102191228,260102195344,260102204898,260102207743,260102207990,260102210457,260102223278,260102225034,260102235058,260102237443,260102248507,260102252996,260102265261,260102307188,260102328937,260102331048,260102338639,260102341195,260102345378,260102352218,260102371333,260102379823,260102382413,260102387263,260102403144,260102403219,260102409299,260102412996,260102414786,260102430295,260102431657,260102436938,260102459393,260102464369,260102464393,260102465341,260102466794,260102470580,260102475290,260102494739,260102496478,260102497369,260102528585,260102530664,260102536026,260102538774,260102539509,260102543469,260102544707,260102557030,260102557485,260102559176,260102572765,260102575453,260102581295,260102586864,260102587136,260102589678,260102593613,260102605151,260102607736,260102609112,260102610409,260102612009,260102612868,260102615663,260102620622,260102629839,260102631231,260102645546,260102648243,260102649480,260102651254,260102654357,260102654456,260102656345,260102664737,260102673324,260102677812,260102682283,260102684719,260102686136,260102688728,260102689452,260102702362,260102706744,260102709722,260102711470,260102712015,260102712957,260102715257,260102716347,260102723277,260102730322,260102735792,260102738978,260102741105,260102741352,260102741618,260102746906,260102756236,260102761350,260102764115,260102764842,260102767969,260102775970,260102777802,260102782166,260102784881,260102786985,260102796406,260102811015,260102822996,260102825593,260102834678,260102841715,260102844230,260102845880,260102847142,260102852399,260102853512,260102856051,260102860061,260102860764,260102861796,260102869427,260102878626,260102879376,260102880325,260102881216,260102893708,260102894193,260102894482,260102895588,260102896727,260102896818,260102897436,260102901022,260102903242,260102904257,260102908910,260102920758,260102922234,260102925906,260102927936,260102933967,260102935152,260102935665,260102937646,260102940749,260102944782,260102950987,260102958824,260102969052,260102969227,260102976008,260102976552,260102985454,260102988862,260102992146,260102999356,260103007449,260103013207,260103016051,260103020848,260103030961,260103034484,260103038394,260103042271,260103050696,260103053625,260103057303,260103065256,260103065827,260103067997,260103070389,260103077293,260103079406,260103080230,260103080792,260103081824,260103084158,260103087045,260103089124,260103089983,260103093001,260103093860,260103094009,260103097788,260103103453,260103107918,260103110615,260103118642,260103123246,260103128765,260103134425,260103139218,260103141008,260103143533,260103148508,260103150611,260103151759,260103157566,260103157806,260103161147,260103162673,260103173456,260103175758,260103176095,260103182010,260103186631,260103188900,260103188991,260103189221,260103191615,260103195780,260103198115,260103198909,260103205662,260103209011,260103209912,260103210647,260103210936,260103211025,260103221248,260103223459,260103226858,260103234050,260103237087,260103237681,260103237988,260103241949,260103243390,260103248258,260103260634,260103262895,260103265419,260103265427,260103266730,260103267712,260103268140,260103271045,260103272167,260103277505,260103280988,260103282505,260103283297,260103289997,260103292975,260103293866,260103301289,260103301339,260103304457,260103305041,260103307070,260103309514,260103317863,260103319406,260103325353,260103329231,260103330106,260103332144,260103335451,260103336327,260103340774,260103348033,260103354288,260103354999,260103356002,260103359832,260103373593,260103379780,260103384699,260103386470,260103389102,260103395547,260103397675,260103398392,260103400768,260103402368,260103405452,260103406328,260103408100,260103408514,260103413860,260103417127,260103425278,260103426037,260103426565,260103429692,260103432092,260103436945,260103439691,260103447058,260103451738,260103451977,260103453825,260103455358,260103456737,260103458089,260103458907,260103459855,260103462230,260103468567,260103470423,260103472205,260103472759,260103479002,260103479085,260103483137,260103485538,260103487575,260103491189,260103496592,260103502431,260103504478,260103504981,260103505632,260103514576,260103518254,260103518460,260103521019,260103526729,260103538658,260103541306,260103542734,260103544771,260103545703,260103548814,260103550547,260103551339,260103553368,260103553475,260103554051,260103557989,260103560777,260103565123,260103567525,260103577904,260103581005,260103584439,260103586145,260103586467,260103590253,260103590642,260103592283,260103594362,260103596292,260103598280,260103599858,260103604393,260103604799,260103606208,260103607107,260103616355,260103625117,260103626222,260103628954,260103631198,260103638979,260103641379,260103648135,260103648218,260103649257,260103651386,260103656997,260103659173,260103667499,260103668398,260103671160,260103671756,260103672424,260103675526,260103679080,260103679874,260103685079,260103688180,260103690541,260103692463,260103693768,260103696092,260103698254,260103704623,260103707816,260103708251,260103711263,260103713665,260103720280,260103726683,260103749677,260103750576,260103757118,260103757365,260103766440,260103768149,260103772570,260103774154,260103776571,260103793972,260103794178,260103795019,260103797387,260103802609,260103806774,260103806956,260103808663,260103813499,260103821872,260103821955,260103823423,260103824538,260103824884,260103842183,260103845145,260103847737,260103851119,260103855425,260103856225,260103857298,260103858858,260103860524,260103861779,260103861795,260103862413,260103869400,260103871968,260103873329,260103874699,260103875076,260103877338,260103880050,260103882023,260103882445,260103883732,260103885190,260103887022,260103887238,260103889606,260103889622,260103890471,260103892709,260103894291,260103901393,260103914198,260103915666,260103917092,260103920526,260103922761,260103924858,260103927539,260103928206,260103929634,260103939609,260103939740,260103941316,260103947388,260103948832,260103949756,260103950655,260103951752,260103953303,260103957395,260103958369,260103961462,260103964284,260103969002,260103972972,260103974143,260103976346,260103980819,260103983045,260103986659,260103988135,260103989851,260103991873,260103992988,260103993242,260103998316,260104005095,260104005533,260104006309,260104010269,260104010541,260104011671,260104012182,260104012398,260104012844,260104013081,260104013586,260104017835,260104019005,260104021381,260104022066,260104022488,260104025424,260104027222,260104029756,260104035092,260104042734,260104043393,260104047691,260104047873,260104050604,260104060330,260104071931,260104076690,260104078589,260104079140,260104079215,260104079843,260104080676,260104081310,260104082300,260104090097,260104091061,260104102421,260104108030,260104108501,260104112313,260104113758,260104124078,260104200902,260104371158,260104413729,260104565577]\n",
    "trouble = df[df['PRN'].isin(listush)]\n",
    "trouble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trouble[trouble['PREDICTION']==50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prn_cols = '''SELECT BANK_ACCOUNT_ID,\n",
    "        MAX(BANK_ACCOUNT_NUMBER) AS BANK_ACCOUNT_NUMBER\n",
    "        FROM \"LILI_ANALYTICS\".\"ODS\".\"MYSQL_DW_CUSTOMER_MONTHLY_NEW\"\n",
    "GROUP BY BANK_ACCOUNT_ID\n",
    "'''\n",
    "\n",
    "prn = pd.read_sql(prn_cols, conn)\n",
    "\n",
    "prn = prn.rename(columns={'BANK_ACCOUNT_NUMBER': 'PRN'})\n",
    "\n",
    "prn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prn['PRN'] = prn['PRN'].astype(int)\n",
    "combo = df.merge(prn, on=['PRN'], how='left')\n",
    "combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bads = combo[combo['PRN'].isin(listush)]\n",
    "bad_ids = list(bads['BANK_ACCOUNT_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[\", end =\" \"),\n",
    "for elt in bad_ids:\n",
    "    print(elt, end =\" \"),\n",
    "    print(\",\", end =\" \")\n",
    "print(\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_ids = [ 100166 , 100182 , 101098 , 101484 , 101856 , 102035 , 102693 , 102760 , 102841 , 102949 , 103757 , 103760 , 103916 , 104245 , 104481 , 105351 , 106040 , 106597 , 107323 , 108575 , 108725 , 109459 , 110105 , 110317 , 111612 , 111943 , 113202 , 113495 , 114948 , 115129 , 115811 , 116575 , 11667 , 117492 , 118310 , 119252 , 119634 , 119795 , 121493 , 122721 , 122880 , 123468 , 124933 , 125868 , 126558 , 126789 , 126832 , 127073 , 127781 , 127898 , 128415 , 130228 , 130430 , 130727 , 131395 , 131608 , 131629 , 131873 , 131937 , 132560 , 132578 , 133628 , 133790 , 134388 , 134573 , 134696 , 135417 , 136060 , 136936 , 138448 , 139484 , 139525 , 14050 , 140539 , 140735 , 140796 , 141181 , 141371 , 141849 , 141999 , 145199 , 145611 , 146566 , 146851 , 146876 , 147122 , 148404 , 148580 , 149582 , 149821 , 150927 , 151376 , 152603 , 156903 , 15844 , 159114 , 159325 , 160084 , 160340 , 160758 , 161442 , 163443 , 164292 , 164551 , 165036 , 166714 , 166721 , 167329 , 167699 , 167878 , 169429 , 169565 , 16958 , 170093 , 172375 , 172872 , 172875 , 172970 , 173115 , 173494 , 173965 , 175909 , 176083 , 176172 , 179345 , 179553 , 180089 , 180364 , 180437 , 180833 , 180957 , 182190 , 182235 , 182404 , 183763 , 184032 , 184616 , 18500 , 185173 , 185200 , 185454 , 185848 , 187002 , 187260 , 187398 , 187527 , 187687 , 187773 , 188053 , 188544 , 189465 , 189605 , 19037 , 191036 , 191306 , 191430 , 191607 , 191917 , 191927 , 192116 , 192955 , 193814 , 194263 , 194710 , 194953 , 195095 , 195354 , 195427 , 196718 , 197156 , 197454 , 197629 , 197683 , 197777 , 198007 , 198116 , 198809 , 19888 , 199514 , 200061 , 200379 , 200592 , 200617 , 200643 , 201197 , 202130 , 202642 , 202918 , 202991 , 203304 , 204105 , 204288 , 204724 , 204996 , 205206 , 206148 , 207609 , 208807 , 209067 , 209975 , 210679 , 210931 , 211096 , 211222 , 211747 , 211859 , 212113 , 212514 , 212584 , 212687 , 213450 , 21352 , 214370 , 214445 , 214540 , 214629 , 215878 , 215927 , 215956 , 216066 , 216180 , 216189 , 216251 , 216610 , 216832 , 216933 , 217399 , 218656 , 218804 , 219171 , 219374 , 219977 , 220096 , 220147 , 220345 , 220655 , 221059 , 221679 , 222463 , 223486 , 223503 , 224181 , 224236 , 225126 , 225467 , 225795 , 226516 , 227325 , 227901 , 228186 , 228665 , 229677 , 230029 , 230420 , 230808 , 231650 , 231943 , 232311 , 233106 , 233163 , 233380 , 233619 , 234310 , 234521 , 234604 , 234660 , 234763 , 23477 , 234996 , 235285 , 235493 , 235579 , 235881 , 235967 , 235981 , 236359 , 236926 , 237372 , 237642 , 238445 , 238905 , 23938 , 239457 , 240023 , 240502 , 240681 , 240934 , 241431 , 241642 , 241756 , 242337 , 242361 , 242695 , 242848 , 243926 , 244156 , 244190 , 244782 , 245244 , 245471 , 245480 , 245503 , 245742 , 246159 , 246392 , 246471 , 247147 , 247482 , 247572 , 247645 , 247674 , 247683 , 248759 , 248980 , 249320 , 250040 , 250343 , 250403 , 250433 , 250829 , 250974 , 251460 , 252698 , 252924 , 253176 , 253177 , 253308 , 253406 , 253449 , 253739 , 253851 , 25431 , 254385 , 254733 , 254885 , 254964 , 255634 , 255932 , 256021 , 256763 , 256768 , 257084 , 257140 , 257212 , 257415 , 257659 , 258494 , 259342 , 259730 , 259817 , 260021 , 260352 , 260439 , 260884 , 261610 , 262235 , 262306 , 262407 , 262790 , 264166 , 264785 , 265276 , 265454 , 265717 , 266361 , 266574 , 266646 , 266883 , 267043 , 267352 , 267439 , 267617 , 267658 , 268193 , 268519 , 269334 , 269410 , 269463 , 269776 , 270016 , 270501 , 270776 , 271512 , 271980 , 272004 , 272189 , 272342 , 272480 , 272615 , 272697 , 272792 , 273030 , 273663 , 273849 , 274027 , 27405 , 274082 , 274707 , 274715 , 275120 , 275360 , 275564 , 275925 , 276466 , 277050 , 277254 , 277305 , 277370 , 278264 , 278632 , 278653 , 278908 , 279479 , 280672 , 280937 , 281080 , 281284 , 281377 , 281688 , 281861 , 281940 , 282143 , 282154 , 282212 , 282605 , 282884 , 283400 , 283640 , 284678 , 284988 , 285331 , 285502 , 285534 , 285913 , 285952 , 286116 , 286324 , 286517 , 286716 , 286873 , 287327 , 287367 , 287508 , 287598 , 288523 , 289399 , 289510 , 289783 , 290007 , 29001 , 290785 , 291025 , 291701 , 291709 , 291813 , 292026 , 292587 , 292805 , 293637 , 293727 , 294004 , 294063 , 294130 , 294440 , 294796 , 294875 , 295395 , 295706 , 295942 , 296134 , 296264 , 296497 , 296713 , 297350 , 297669 , 297713 , 298014 , 298254 , 298916 , 299556 , 301855 , 301945 , 302599 , 302624 , 303532 , 303702 , 304145 , 304303 , 304545 , 306285 , 306305 , 306389 , 306626 , 307148 , 30730 , 307565 , 307583 , 307754 , 308237 , 309122 , 309130 , 309277 , 309388 , 309423 , 311153 , 311449 , 311708 , 312144 , 312575 , 312655 , 312762 , 312918 , 313085 , 313210 , 313212 , 313274 , 313973 , 314229 , 314365 , 314502 , 314540 , 314766 , 315038 , 315235 , 315277 , 315406 , 315552 , 315735 , 315756 , 315993 , 315995 , 316080 , 316303 , 316462 , 317172 , 318452 , 318599 , 318742 , 319085 , 319309 , 319518 , 319786 , 319853 , 319996 , 320993 , 321007 , 321164 , 321724 , 321869 , 321961 , 322051 , 322161 , 322316 , 322725 , 322822 , 323132 , 323414 , 323886 , 324283 , 324400 , 324620 , 325067 , 32517 , 325290 , 325651 , 325799 , 325971 , 326173 , 326284 , 326310 , 326817 , 327495 , 327539 , 327616 , 328012 , 328040 , 328153 , 328204 , 328225 , 328270 , 328294 , 328344 , 328769 , 328886 , 329124 , 329192 , 329234 , 329528 , 329708 , 329961 , 330495 , 331259 , 331325 , 331755 , 331773 , 332046 , 333019 , 334179 , 334655 , 334844 , 334900 , 334907 , 334970 , 335053 , 335117 , 335216 , 335995 , 336092 , 337228 , 337789 , 337836 , 338217 , 338361 , 339393 , 34456 , 347076 , 3491 , 364187 , 368444 , 36884 , 37336 , 3734 , 383629 , 39490 , 39643 , 41062 , 41370 , 43695 , 44389 , 44960 , 45117 , 45118 , 45611 , 46470 , 46955 , 47196 , 47835 , 48212 , 48911 , 48987 , 49428 , 50936 , 51567 , 5346 , 54927 , 54974 , 55082 , 55197 , 55585 , 55916 , 55987 , 57349 , 57392 , 57418 , 59595 , 60402 , 61539 , 62057 , 62936 , 63094 , 64248 , 64624 , 65088 , 66424 , 6852 , 72822 , 76213 , 77150 , 77424 , 79197 , 80108 , 80202 , 8022 , 80719 , 81526 , 82312 , 8348 , 83503 , 83926 , 84012 , 84535 , 86458 , 87008 , 87729 , 87877 , 88413 , 88446 , 88497 , 89442 , 89598 , 90909 , 91547 , 91626 , 95727 , 96243 , 97455]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bad_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odlims_cols = '''select * from \"LILI_ANALYTICS_DEV\".\"ODS\".\"OVERDRAFT_LIMITS\"\n",
    "'''\n",
    "\n",
    "odlims = pd.read_sql(odlims_cols, conn)\n",
    "\n",
    "odlims = odlims[odlims['BANK_ACCOUNT_NUMBER'].isin(listush)]\n",
    "odlims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = odlims['EXPIRATION_DATE'].value_counts()\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elt in dates.keys():\n",
    "    print(f\"'{elt}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odlims[odlims['MODEL_REASON']=='model rejection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(odlims[['BANK_ACCOUNT_NUMBER', 'OVERDRAFT_LIMIT']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dict = dict({\n",
    "0: 'TOTAL_MONEY_IN',\n",
    "1: 'TOTAL_MONEY_IN_COUNT',\n",
    "2: 'TOTAL_MONEY_IN_WEEK1',\n",
    "3: 'TOTAL_MONEY_IN_COUNT_WEEK1',\n",
    "4: 'TOTAL_MONEY_IN_WEEK2',\n",
    "5: 'TOTAL_MONEY_IN_COUNT_WEEK2',\n",
    "6: 'TOTAL_MONEY_IN_WEEK3',\n",
    "7: 'TOTAL_MONEY_IN_COUNT_WEEK3',\n",
    "8: 'TOTAL_MONEY_IN_WEEK4',\n",
    "9: 'TOTAL_MONEY_IN_COUNT_WEEK4',\n",
    "10: 'HAD_NEGATIVE_BALANCE',\n",
    "11: 'ATM_SUM',\n",
    "12: 'SWIPE_SUM',\n",
    "13: 'SPEND_SUM',\n",
    "14: 'DIRECT_DEPOSIT_SUM',\n",
    "15: 'DIRECT_PAY_SUM',\n",
    "16: 'ACH_SUM',\n",
    "17: 'CHECK_SUM',\n",
    "18: 'GREENDOT_SUM',\n",
    "19: 'CARD_DEPOSIT_SUM',\n",
    "20: 'AVERAGE_BALANCE',\n",
    "21: 'DID_PAYROLL',\n",
    "22: 'DID_MARKETPLACE',\n",
    "23: 'DID_FINANCIAL_INSTITUTION',\n",
    "24: 'DID_UNEMPLOYMENT',\n",
    "25: 'DID_TAX_REFUND',\n",
    "26: 'DID_NONE',\n",
    "27: 'LOGIN_COUNT',\n",
    "28: 'PENDING_CHECK',\n",
    "29: 'DENIED_AUTH_NSF',\n",
    "30: 'DENIED_AUTH_NSF_CNT',\n",
    "31: 'DENIED_AUTH',\n",
    "32: 'DENIED_AUTH_CNT',\n",
    "33: 'AUTH_EXP',\n",
    "34: 'AUTH_EXP_CNT',\n",
    "35: 'DENIED_AUTH_INACTIVE_CARD',\n",
    "36: 'DENIED_AUTH_INACTIVE_CARD_CNT',\n",
    "37: 'DENIED_AUTH_INVALID_PIN',\n",
    "38: 'DENIED_AUTH_INVALID_PIN_CNT',\n",
    "39: 'DENIED_AUTH_GAS',\n",
    "40: 'DENIED_AUTH_GAS_CNT',\n",
    "41: 'ACH_CREDIT_FAIL',\n",
    "42: 'ACH_CREDIT_FAIL_CNT',\n",
    "43: 'ACH_CREDIT_RETURN',\n",
    "44: 'ACH_CREDIT_RETURN_CNT',\n",
    "45: 'CREATE_HOLD',\n",
    "46: 'CREATE_HOLD_CNT',\n",
    "47: 'EXPIRE_HOLD',\n",
    "48: 'EXPIRE_HOLD_CNT',\n",
    "49: 'TIME_FROM_LAST_TRANSACTION',\n",
    "50: 'TIME_FROM_LAST_MONEY_IN',\n",
    "51: 'AVG_MONEY_IN',\n",
    "52: '0742',\n",
    "53: '0763',\n",
    "54: '0780',\n",
    "55: '1520',\n",
    "56: '1711',\n",
    "57: '1731',\n",
    "58: '1740',\n",
    "59: '1750',\n",
    "60: '1761',\n",
    "61: '1771',\n",
    "62: '1799',\n",
    "63: '2741',\n",
    "64: '2791',\n",
    "65: '2842',\n",
    "66: '3000',\n",
    "67: '3001',\n",
    "68: '3005',\n",
    "69: '3007',\n",
    "70: '3008',\n",
    "71: '3009',\n",
    "72: '3010',\n",
    "73: '3022',\n",
    "74: '3026',\n",
    "75: '3032',\n",
    "76: '3035',\n",
    "77: '3037',\n",
    "78: '3039',\n",
    "79: '3047',\n",
    "80: '3050',\n",
    "81: '3058',\n",
    "82: '3061',\n",
    "83: '3066',\n",
    "84: '3069',\n",
    "85: '3076',\n",
    "86: '3084',\n",
    "87: '3098',\n",
    "88: '3102',\n",
    "89: '3132',\n",
    "90: '3136',\n",
    "91: '3144',\n",
    "92: '3174',\n",
    "93: '3175',\n",
    "94: '3196',\n",
    "95: '3219',\n",
    "96: '3256',\n",
    "97: '3260',\n",
    "98: '3351',\n",
    "99: '3355',\n",
    "100: '3357',\n",
    "101: '3359',\n",
    "102: '3366',\n",
    "103: '3370',\n",
    "104: '3386',\n",
    "105: '3387',\n",
    "106: '3389',\n",
    "107: '3390',\n",
    "108: '3393',\n",
    "109: '3395',\n",
    "110: '3405',\n",
    "111: '3441',\n",
    "112: '3501',\n",
    "113: '3502',\n",
    "114: '3503',\n",
    "115: '3504',\n",
    "116: '3508',\n",
    "117: '3509',\n",
    "118: '3510',\n",
    "119: '3512',\n",
    "120: '3513',\n",
    "121: '3515',\n",
    "122: '3516',\n",
    "123: '3519',\n",
    "124: '3520',\n",
    "125: '3523',\n",
    "126: '3526',\n",
    "127: '3527',\n",
    "128: '3528',\n",
    "129: '3530',\n",
    "130: '3532',\n",
    "131: '3535',\n",
    "132: '3542',\n",
    "133: '3543',\n",
    "134: '3551',\n",
    "135: '3553',\n",
    "136: '3555',\n",
    "137: '3558',\n",
    "138: '3559',\n",
    "139: '3561',\n",
    "140: '3562',\n",
    "141: '3563',\n",
    "142: '3564',\n",
    "143: '3565',\n",
    "144: '3567',\n",
    "145: '3575',\n",
    "146: '3576',\n",
    "147: '3581',\n",
    "148: '3583',\n",
    "149: '3584',\n",
    "150: '3588',\n",
    "151: '3589',\n",
    "152: '3590',\n",
    "153: '3591',\n",
    "154: '3592',\n",
    "155: '3595',\n",
    "156: '3596',\n",
    "157: '3597',\n",
    "158: '3604',\n",
    "159: '3607',\n",
    "160: '3608',\n",
    "161: '3609',\n",
    "162: '3613',\n",
    "163: '3614',\n",
    "164: '3615',\n",
    "165: '3617',\n",
    "166: '3618',\n",
    "167: '3619',\n",
    "168: '3621',\n",
    "169: '3626',\n",
    "170: '3627',\n",
    "171: '3628',\n",
    "172: '3629',\n",
    "173: '3631',\n",
    "174: '3634',\n",
    "175: '3637',\n",
    "176: '3638',\n",
    "177: '3640',\n",
    "178: '3641',\n",
    "179: '3644',\n",
    "180: '3649',\n",
    "181: '3650',\n",
    "182: '3652',\n",
    "183: '3654',\n",
    "184: '3660',\n",
    "185: '3662',\n",
    "186: '3665',\n",
    "187: '3667',\n",
    "188: '3670',\n",
    "189: '3671',\n",
    "190: '3676',\n",
    "191: '3679',\n",
    "192: '3684',\n",
    "193: '3685',\n",
    "194: '3687',\n",
    "195: '3690',\n",
    "196: '3692',\n",
    "197: '3693',\n",
    "198: '3694',\n",
    "199: '3695',\n",
    "200: '3697',\n",
    "201: '3700',\n",
    "202: '3703',\n",
    "203: '3704',\n",
    "204: '3705',\n",
    "205: '3706',\n",
    "206: '3708',\n",
    "207: '3709',\n",
    "208: '3710',\n",
    "209: '3715',\n",
    "210: '3717',\n",
    "211: '3721',\n",
    "212: '3722',\n",
    "213: '3726',\n",
    "214: '3728',\n",
    "215: '3730',\n",
    "216: '3731',\n",
    "217: '3734',\n",
    "218: '3735',\n",
    "219: '3738',\n",
    "220: '3740',\n",
    "221: '3741',\n",
    "222: '3742',\n",
    "223: '3745',\n",
    "224: '3750',\n",
    "225: '3751',\n",
    "226: '3763',\n",
    "227: '3764',\n",
    "228: '3765',\n",
    "229: '3769',\n",
    "230: '3770',\n",
    "231: '3771',\n",
    "232: '3773',\n",
    "233: '3774',\n",
    "234: '3775',\n",
    "235: '3777',\n",
    "236: '3778',\n",
    "237: '3779',\n",
    "238: '3780',\n",
    "239: '3782',\n",
    "240: '3785',\n",
    "241: '3786',\n",
    "242: '3816',\n",
    "243: '4111',\n",
    "244: '4112',\n",
    "245: '4119',\n",
    "246: '4121',\n",
    "247: '4131',\n",
    "248: '4214',\n",
    "249: '4215',\n",
    "250: '4225',\n",
    "251: '4411',\n",
    "252: '4457',\n",
    "253: '4468',\n",
    "254: '4511',\n",
    "255: '4582',\n",
    "256: '4722',\n",
    "257: '4784',\n",
    "258: '4789',\n",
    "259: '4812',\n",
    "260: '4814',\n",
    "261: '4816',\n",
    "262: '4821',\n",
    "263: '4829',\n",
    "264: '4899',\n",
    "265: '4900',\n",
    "266: '5013',\n",
    "267: '5021',\n",
    "268: '5039',\n",
    "269: '5044',\n",
    "270: '5045',\n",
    "271: '5046',\n",
    "272: '5047',\n",
    "273: '5051',\n",
    "274: '5065',\n",
    "275: '5072',\n",
    "276: '5074',\n",
    "277: '5085',\n",
    "278: '5094',\n",
    "279: '5099',\n",
    "280: '5111',\n",
    "281: '5122',\n",
    "282: '5131',\n",
    "283: '5137',\n",
    "284: '5139',\n",
    "285: '5169',\n",
    "286: '5172',\n",
    "287: '5192',\n",
    "288: '5193',\n",
    "289: '5198',\n",
    "290: '5199',\n",
    "291: '5200',\n",
    "292: '5211',\n",
    "293: '5231',\n",
    "294: '5251',\n",
    "295: '5261',\n",
    "296: '5271',\n",
    "297: '5300',\n",
    "298: '5309',\n",
    "299: '5310',\n",
    "300: '5311',\n",
    "301: '5331',\n",
    "302: '5399',\n",
    "303: '5411',\n",
    "304: '5422',\n",
    "305: '5441',\n",
    "306: '5451',\n",
    "307: '5462',\n",
    "308: '5499',\n",
    "309: '5511',\n",
    "310: '5521',\n",
    "311: '5532',\n",
    "312: '5533',\n",
    "313: '5541',\n",
    "314: '5542',\n",
    "315: '5551',\n",
    "316: '5561',\n",
    "317: '5571',\n",
    "318: '5598',\n",
    "319: '5599',\n",
    "320: '5611',\n",
    "321: '5621',\n",
    "322: '5631',\n",
    "323: '5641',\n",
    "324: '5651',\n",
    "325: '5655',\n",
    "326: '5661',\n",
    "327: '5681',\n",
    "328: '5691',\n",
    "329: '5697',\n",
    "330: '5698',\n",
    "331: '5699',\n",
    "332: '5712',\n",
    "333: '5713',\n",
    "334: '5714',\n",
    "335: '5718',\n",
    "336: '5719',\n",
    "337: '5722',\n",
    "338: '5732',\n",
    "339: '5733',\n",
    "340: '5734',\n",
    "341: '5735',\n",
    "342: '5811',\n",
    "343: '5812',\n",
    "344: '5813',\n",
    "345: '5814',\n",
    "346: '5815',\n",
    "347: '5816',\n",
    "348: '5817',\n",
    "349: '5818',\n",
    "350: '5912',\n",
    "351: '5921',\n",
    "352: '5931',\n",
    "353: '5932',\n",
    "354: '5933',\n",
    "355: '5935',\n",
    "356: '5937',\n",
    "357: '5940',\n",
    "358: '5941',\n",
    "359: '5942',\n",
    "360: '5943',\n",
    "361: '5944',\n",
    "362: '5945',\n",
    "363: '5946',\n",
    "364: '5947',\n",
    "365: '5948',\n",
    "366: '5949',\n",
    "367: '5950',\n",
    "368: '5960',\n",
    "369: '5962',\n",
    "370: '5963',\n",
    "371: '5964',\n",
    "372: '5965',\n",
    "373: '5966',\n",
    "374: '5967',\n",
    "375: '5968',\n",
    "376: '5969',\n",
    "377: '5970',\n",
    "378: '5971',\n",
    "379: '5972',\n",
    "380: '5973',\n",
    "381: '5975',\n",
    "382: '5976',\n",
    "383: '5977',\n",
    "384: '5978',\n",
    "385: '5983',\n",
    "386: '5992',\n",
    "387: '5993',\n",
    "388: '5994',\n",
    "389: '5995',\n",
    "390: '5996',\n",
    "391: '5997',\n",
    "392: '5998',\n",
    "393: '5999',\n",
    "394: '6010',\n",
    "395: '6011',\n",
    "396: '6012',\n",
    "397: '6051',\n",
    "398: '6211',\n",
    "399: '6300',\n",
    "400: '6513',\n",
    "401: '7011',\n",
    "402: '7012',\n",
    "403: '7032',\n",
    "404: '7033',\n",
    "405: '7210',\n",
    "406: '7211',\n",
    "407: '7216',\n",
    "408: '7217',\n",
    "409: '7221',\n",
    "410: '7230',\n",
    "411: '7251',\n",
    "412: '7261',\n",
    "413: '7273',\n",
    "414: '7276',\n",
    "415: '7277',\n",
    "416: '7278',\n",
    "417: '7296',\n",
    "418: '7297',\n",
    "419: '7298',\n",
    "420: '7299',\n",
    "421: '7311',\n",
    "422: '7321',\n",
    "423: '7333',\n",
    "424: '7338',\n",
    "425: '7339',\n",
    "426: '7342',\n",
    "427: '7349',\n",
    "428: '7361',\n",
    "429: '7372',\n",
    "430: '7375',\n",
    "431: '7379',\n",
    "432: '7392',\n",
    "433: '7393',\n",
    "434: '7394',\n",
    "435: '7395',\n",
    "436: '7399',\n",
    "437: '7512',\n",
    "438: '7513',\n",
    "439: '7519',\n",
    "440: '7523',\n",
    "441: '7531',\n",
    "442: '7534',\n",
    "443: '7535',\n",
    "444: '7538',\n",
    "445: '7542',\n",
    "446: '7549',\n",
    "447: '7622',\n",
    "448: '7623',\n",
    "449: '7629',\n",
    "450: '7631',\n",
    "451: '7641',\n",
    "452: '7692',\n",
    "453: '7699',\n",
    "454: '7800',\n",
    "455: '7801',\n",
    "456: '7802',\n",
    "457: '7829',\n",
    "458: '7832',\n",
    "459: '7841',\n",
    "460: '7911',\n",
    "461: '7922',\n",
    "462: '7929',\n",
    "463: '7932',\n",
    "464: '7933',\n",
    "465: '7941',\n",
    "466: '7991',\n",
    "467: '7992',\n",
    "468: '7993',\n",
    "469: '7994',\n",
    "470: '7995',\n",
    "471: '7996',\n",
    "472: '7997',\n",
    "473: '7998',\n",
    "474: '7999',\n",
    "475: '8011',\n",
    "476: '8021',\n",
    "477: '8041',\n",
    "478: '8042',\n",
    "479: '8043',\n",
    "480: '8049',\n",
    "481: '8050',\n",
    "482: '8062',\n",
    "483: '8071',\n",
    "484: '8099',\n",
    "485: '8111',\n",
    "486: '8211',\n",
    "487: '8220',\n",
    "488: '8241',\n",
    "489: '8244',\n",
    "490: '8249',\n",
    "491: '8299',\n",
    "492: '8351',\n",
    "493: '8398',\n",
    "494: '8641',\n",
    "495: '8651',\n",
    "496: '8661',\n",
    "497: '8675',\n",
    "498: '8699',\n",
    "499: '8734',\n",
    "500: '8911',\n",
    "501: '8931',\n",
    "502: '8999',\n",
    "503: '9211',\n",
    "504: '9222',\n",
    "505: '9223',\n",
    "506: '9311',\n",
    "507: '9399',\n",
    "508: '9402',\n",
    "509: '9405'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(feature_list)):\n",
    "    if Dict[i]!=feature_list[i]:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [-1,-2,-3,-4]\n",
    "\n",
    "lst = np.abs(lst)\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../model_dumps/131021.csv', header=None)\n",
    "lst = df[3].value_counts()\n",
    "for i in range(0, lst.keys().shape[0]):\n",
    "    print(lst.keys()[i] + \"- amount: \" + str(lst[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [[1,2,1],[3,5,3], [6,4,7]]\n",
    "for elt in lst:\n",
    "    print(elt[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../model_dumps/integ201021.csv', header=None)\n",
    "df = df.rename(columns={0: \"BANK_ACCOUNT_NUMBER\", 1: \"NEW_LIMIT\", 2: \"NEW_EXP\", 3: \"NEW_REASON\"})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['NEW_LIMIT']==0) & (df['NEW_REASON']!='not recurrent and not close expiration')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OD_cols = ''' select *  from \"LILI_ANALYTICS_DEV\".\"ODS\".\"OVERDRAFT_LIMITS\" '''\n",
    "\n",
    "OD_table = pd.read_sql(OD_cols, conn)\n",
    "OD_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = df.merge(OD_table, on=['BANK_ACCOUNT_NUMBER'], how='left')\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prns = list(merged[(merged['NEW_LIMIT']==0) & (merged['OVERDRAFT_LIMIT']!='0.0000') & (merged['NEW_REASON']!='not recurrent and not close expiration')]['BANK_ACCOUNT_NUMBER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [260101892867, 260101919967, 260101978336, 260102095411, 260102159118, 260102374618, 260102564309, 260102743085, 260103052189, 260103381257, 260103620860, 260104063136, 260104070503, 260104217765, 260104274386, 260104274501, 260104388848, 260104492277, 260104730023, 260101419950, 260101029817, 260101509727, 260101519791, 260102289709]\n",
    "df[df[0].isin(lst)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "querush  =   '''select * from \n",
    "(SELECT BANK_ACCOUNT_ID,\n",
    "        MAX(BANK_ACCOUNT_NUMBER) AS BANK_ACCOUNT_NUMBER,\n",
    "        MAX((CASE WHEN PRODUCT_ID = 20643 THEN 1 ELSE 0 END)) AS IS_PRO_SINCE_SIGNUP\n",
    "FROM \"LILI_ANALYTICS\".\"ODS\".\"MYSQL_DW_CUSTOMER_MONTHLY_NEW\"\n",
    "WHERE BANK_ACCOUNT_NUMBER in (260101751642,\n",
    " 260101775336,\n",
    " 260101796415,\n",
    " 260101846491,\n",
    " 260101848976,\n",
    " 260101857829,\n",
    " 260101878239,\n",
    " 260101878262,\n",
    " 260101900116,\n",
    " 260101082295,\n",
    " 260101906618,\n",
    " 260101925519,\n",
    " 260101947430,\n",
    " 260101983468,\n",
    " 260102062650,\n",
    " 260102068384,\n",
    " 260102085917,\n",
    " 260102125929,\n",
    " 260102138195,\n",
    " 260102146917,\n",
    " 260102170271,\n",
    " 260102180494,\n",
    " 260102192465,\n",
    " 260102202561,\n",
    " 260102204351,\n",
    " 260102204740,\n",
    " 260102207057,\n",
    " 260102216686,\n",
    " 260102234135,\n",
    " 260102235124,\n",
    " 260102316049,\n",
    " 260102364114,\n",
    " 260102365889,\n",
    " 260101121671,\n",
    " 260102382413,\n",
    " 260102402534,\n",
    " 260102413739,\n",
    " 260102443231,\n",
    " 260102446556,\n",
    " 260102490430,\n",
    " 260102504677,\n",
    " 260102508264,\n",
    " 260102525763,\n",
    " 260101138204,\n",
    " 260102544400,\n",
    " 260102554698,\n",
    " 260102571916,\n",
    " 260102573441,\n",
    " 260102577582,\n",
    " 260102608890,\n",
    " 260102612009,\n",
    " 260102614492,\n",
    " 260102640398,\n",
    " 260102645025,\n",
    " 260102647104,\n",
    " 260102647740,\n",
    " 260102657509,\n",
    " 260102666666,\n",
    " 260102674462,\n",
    " 260102705605,\n",
    " 260102707932,\n",
    " 260102729837,\n",
    " 260102764446,\n",
    " 260102773355,\n",
    " 260102796513,\n",
    " 260102825452,\n",
    " 260102827151,\n",
    " 260102842671,\n",
    " 260102875457,\n",
    " 260102898202,\n",
    " 260102907227,\n",
    " 260102913134,\n",
    " 260102934395,\n",
    " 260102963089,\n",
    " 260102974938,\n",
    " 260102978038,\n",
    " 260102981412,\n",
    " 260102994399,\n",
    " 260103000873,\n",
    " 260103010674,\n",
    " 260103013645,\n",
    " 260103032397,\n",
    " 260103062584,\n",
    " 260103068243,\n",
    " 260101190379,\n",
    " 260103090502,\n",
    " 260103097390,\n",
    " 260103103925,\n",
    " 260103105219,\n",
    " 260103113841,\n",
    " 260103126371,\n",
    " 260103128765,\n",
    " 260103167789,\n",
    " 260103184966,\n",
    " 260103191003,\n",
    " 260103203154,\n",
    " 260103235073,\n",
    " 260103251633,\n",
    " 260103256525,\n",
    " 260103272662,\n",
    " 260103298378,\n",
    " 260103329082,\n",
    " 260103341780,\n",
    " 260103350088,\n",
    " 260103353132,\n",
    " 260103364766,\n",
    " 260103372207,\n",
    " 260103376992,\n",
    " 260103378204,\n",
    " 260103379913,\n",
    " 260103381992,\n",
    " 260103385522,\n",
    " 260103399036,\n",
    " 260103408373,\n",
    " 260103410155,\n",
    " 260103414470,\n",
    " 260103444121,\n",
    " 260103451555,\n",
    " 260103455697,\n",
    " 260103474664,\n",
    " 260103482303,\n",
    " 260103484333,\n",
    " 260103488730,\n",
    " 260103515219,\n",
    " 260103516530,\n",
    " 260103523288,\n",
    " 260103525002,\n",
    " 260103529129,\n",
    " 260103531174,\n",
    " 260103541637,\n",
    " 260103552162,\n",
    " 260103558789,\n",
    " 260103570958,\n",
    " 260103574331,\n",
    " 260103575403,\n",
    " 260101239440,\n",
    " 260103587432,\n",
    " 260103624334,\n",
    " 260103648499,\n",
    " 260103658381,\n",
    " 260103682290,\n",
    " 260103702379,\n",
    " 260103708228,\n",
    " 260103709184,\n",
    " 260103740098,\n",
    " 260103744504,\n",
    " 260103745410,\n",
    " 260103767497,\n",
    " 260103806774,\n",
    " 260103823316,\n",
    " 260103838793,\n",
    " 260103861357,\n",
    " 260103870762,\n",
    " 260103871968,\n",
    " 260103876728,\n",
    " 260103878542,\n",
    " 260103933198,\n",
    " 260103961975,\n",
    " 260103964367,\n",
    " 260104003603,\n",
    " 260104012042,\n",
    " 260104038542,\n",
    " 260104043401,\n",
    " 260104044086,\n",
    " 260104045620,\n",
    " 260104058433,\n",
    " 260104072376,\n",
    " 260104077698,\n",
    " 260104079876,\n",
    " 260101289742,\n",
    " 260104090493,\n",
    " 260104091616,\n",
    " 260104116447,\n",
    " 260104133376,\n",
    " 260104144100,\n",
    " 260104155510,\n",
    " 260104162607,\n",
    " 260104176185,\n",
    " 260104188552,\n",
    " 260104194964,\n",
    " 260104200605,\n",
    " 260104200936,\n",
    " 260104218524,\n",
    " 260104226790,\n",
    " 260104233960,\n",
    " 260104245097,\n",
    " 260104248661,\n",
    " 260104283866,\n",
    " 260104289939,\n",
    " 260104300017,\n",
    " 260104305156,\n",
    " 260104323654,\n",
    " 260104328158,\n",
    " 260104332143,\n",
    " 260104355896,\n",
    " 260104375647,\n",
    " 260104404694,\n",
    " 260104436662,\n",
    " 260104450093,\n",
    " 260101326916,\n",
    " 260104462841,\n",
    " 260104464250,\n",
    " 260104464946,\n",
    " 260104465794,\n",
    " 260104468632,\n",
    " 260101327914,\n",
    " 260104474952,\n",
    " 260104479621,\n",
    " 260101330744,\n",
    " 260104535117,\n",
    " 260104536172,\n",
    " 260104543202,\n",
    " 260104546924,\n",
    " 260104564620,\n",
    " 260104564877,\n",
    " 260104592407,\n",
    " 260104594346,\n",
    " 260104622527,\n",
    " 260104632435,\n",
    " 260104636683,\n",
    " 260104638986,\n",
    " 260104640883,\n",
    " 260104657580,\n",
    " 260104672191,\n",
    " 260104676010,\n",
    " 260104686910,\n",
    " 260104704846,\n",
    " 260104750849,\n",
    " 260104783121,\n",
    " 260104813639,\n",
    " 260104916598,\n",
    " 260101371433,\n",
    " 260101427854,\n",
    " 260101442598,\n",
    " 260101447399,\n",
    " 260101447779,\n",
    " 260101464832,\n",
    " 260101471621,\n",
    " 260101494466,\n",
    " 260101496560,\n",
    " 260101503118,\n",
    " 260101516276,\n",
    " 260101517118,\n",
    " 260101552412,\n",
    " 260101558799,\n",
    " 260101562635,\n",
    " 260101569994,\n",
    " 260101581213,\n",
    " 260101585016,\n",
    " 260101594026,\n",
    " 260101638914,\n",
    " 260101649465,                        \n",
    " 260101660785,\n",
    " 260101662997,\n",
    " 260101690550,\n",
    " 260101709111)\n",
    "GROUP BY BANK_ACCOUNT_ID) bank\n",
    "left join(\n",
    "\n",
    "    SELECT t.BANK_ACCOUNT_ID, t.CURRENT_BALANCE\n",
    "FROM \"LILI_ANALYTICS_DEV\".\"ODS\".\"MYSQL_CUSTOMER_BALANCE_HISTORY\" t\n",
    "INNER JOIN (\n",
    "SELECT BANK_ACCOUNT_ID, MAX(CASE WHEN VALID_DATE <=  '2021-10-20' THEN VALID_DATE END) AS MAXDATE\n",
    "FROM \"LILI_ANALYTICS_DEV\".\"ODS\".\"MYSQL_CUSTOMER_BALANCE_HISTORY\" tm\n",
    "GROUP BY BANK_ACCOUNT_ID\n",
    ") tm ON t.BANK_ACCOUNT_ID = tm.BANK_ACCOUNT_ID AND t.VALID_DATE = tm.MAXDATE) dm on bank.BANK_ACCOUNT_ID = dm.BANK_ACCOUNT_ID'''\n",
    "\n",
    "table = pd.read_sql(querush, conn)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table['BANK_ACCOUNT_NUMBER'] = table['BANK_ACCOUNT_NUMBER'].astype('int64')\n",
    "table.dtypes\n",
    "mergush = table.merge(df[['BANK_ACCOUNT_NUMBER', 'NEW_LIMIT']], on=['BANK_ACCOUNT_NUMBER'], how='left')\n",
    "mergush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (airflow-dags)",
   "language": "python",
   "name": "pycharm-8f8523ae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
